{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMWwDgz3JlA0"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgS9K08_fBtr"
      },
      "outputs": [],
      "source": [
        "# Twitter Developer Data\n",
        "my_consumer_key = \"sVGZ5Xb17b9DRDtnjoNyIQbUW\"\n",
        "my_consumer_secret = \"K9x4B5mPCAy3L0XOEtThFSbkI09IFBO927bOq4SdFzKjY9kbvD\"\n",
        "\n",
        "my_access_token = \"897125074842509313-RFAbBbMk7N8LoqI90blUKLCqMf5k0WZ\"\n",
        "my_access_secret = \"6TMDnARwmeU111p49FzN72b6cNobRNfz5930xFlv2Ljj6\"\n",
        "\n",
        "my_bearer_token = \"AAAAAAAAAAAAAAAAAAAAAAinlgEAAAAAouObbp7FqJFz9BuHUoArzhd0za8%3DEEfd9VwOwso847bUQMZgtM9gUQlkQMJjP9CD8VU6Zb2SGR6Aif\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJEMrigzfCmg",
        "outputId": "b7234f53-272c-4833-a95e-3d242d721f6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/tweepy/tweepy.git\n",
            "  Cloning https://github.com/tweepy/tweepy.git to /tmp/pip-req-build-h_xsccet\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/tweepy/tweepy.git /tmp/pip-req-build-h_xsccet\n",
            "  Resolved https://github.com/tweepy/tweepy.git to commit d2a6b83f7da87d66204d0e334ff2d94c21ea7cbc\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from tweepy==4.13.0) (3.2.2)\n",
            "Collecting requests<3,>=2.27.0\n",
            "  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.8/62.8 KB\u001b[0m \u001b[31m775.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests-oauthlib<2,>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from tweepy==4.13.0) (1.3.1)\n",
            "Collecting charset-normalizer<4,>=2\n",
            "  Downloading charset_normalizer-3.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.27.0->tweepy==4.13.0) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.27.0->tweepy==4.13.0) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.27.0->tweepy==4.13.0) (2.10)\n",
            "Building wheels for collected packages: tweepy\n",
            "  Building wheel for tweepy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tweepy: filename=tweepy-4.13.0-py3-none-any.whl size=102783 sha256=59cd4aa0e0794245f6382d052f2181ca2075a3601ba089ab78da83405ce8ec80\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jx_zbvmk/wheels/37/db/50/af7b44cc37a83332e50fbebe7290974b5f463e294dbc4eeaf8\n",
            "Successfully built tweepy\n",
            "Installing collected packages: charset-normalizer, requests, tweepy\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.25.1\n",
            "    Uninstalling requests-2.25.1:\n",
            "      Successfully uninstalled requests-2.25.1\n",
            "  Attempting uninstall: tweepy\n",
            "    Found existing installation: tweepy 3.10.0\n",
            "    Uninstalling tweepy-3.10.0:\n",
            "      Successfully uninstalled tweepy-3.10.0\n",
            "Successfully installed charset-normalizer-3.1.0 requests-2.28.2 tweepy-4.13.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.9/dist-packages (1.8.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from wordcloud) (3.5.3)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.9/dist-packages (from wordcloud) (1.22.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.9/dist-packages (from wordcloud) (8.4.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (4.39.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (23.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (2.8.2)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "# installing the correct version of tweepy\n",
        "!pip install git+https://github.com/tweepy/tweepy.git --upgrade\n",
        "#for words\n",
        "!pip install wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5rA1ghXfFVr"
      },
      "outputs": [],
      "source": [
        "import tweepy # Import the tweepy package\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y16NRNJyfMFy"
      },
      "outputs": [],
      "source": [
        "# Using the tweepy.Client(...) function, you can establish a connection to the Twitter API\n",
        "client = tweepy.Client(\n",
        "    wait_on_rate_limit = True, #helps to overcome a lot of errors associated with API usage limitations.\n",
        "    consumer_key = my_consumer_key, #passing credentials to appropriate parameters\n",
        "    consumer_secret = my_consumer_secret,\n",
        "    access_token = my_access_token,\n",
        "    access_token_secret = my_access_secret,\n",
        "    bearer_token = my_bearer_token,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTDlKYPnfN6L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "048683d9-d3d1-45f2-c8bb-53cfa927abf5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tweepy.client.Client at 0x7f316a73cd30>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Test to make sure the client object exists\n",
        "client"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vo0la09WJfsu"
      },
      "source": [
        "# Collect Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4aTMiUAwwAH"
      },
      "source": [
        "## Brand"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-RTZjgnwx95"
      },
      "outputs": [],
      "source": [
        "#list of companies that are being analyzed and compared\n",
        "brand_list = [\"Starbucks\", \"dunkindonuts\", \"netflix\", \"hulu\", \"SouthwestAir\", \"Delta\"]\n",
        "df_brand = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qw-0OrwwyA0"
      },
      "outputs": [],
      "source": [
        "#iterate through each company to collect its twitter page information\n",
        "for i in brand_list:\n",
        "  brands = client.get_user (\n",
        "      username = i,\n",
        "      user_fields = [\"created_at\", \"description\", \"id\", \"location\", \"name\", \"public_metrics\", \"username\", \"verified\", \"verified_type\"],)\n",
        "  df_brand.append(brands.data.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "oNj9tFmL5wOG",
        "outputId": "e6b2b2f0-605a-4999-b115-0dd15997250d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-c887a1ad-5e83-4b0b-b81e-dbf7f766906c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>username</th>\n",
              "      <th>description</th>\n",
              "      <th>name</th>\n",
              "      <th>verified</th>\n",
              "      <th>location</th>\n",
              "      <th>created_at</th>\n",
              "      <th>id</th>\n",
              "      <th>verified_type</th>\n",
              "      <th>public_metrics_followers_count</th>\n",
              "      <th>public_metrics_following_count</th>\n",
              "      <th>public_metrics_tweet_count</th>\n",
              "      <th>public_metrics_listed_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Starbucks</td>\n",
              "      <td>Make today a good day. üíö</td>\n",
              "      <td>Starbucks Coffee</td>\n",
              "      <td>True</td>\n",
              "      <td>Seattle, WA</td>\n",
              "      <td>2006-11-29T19:19:02.000Z</td>\n",
              "      <td>30973</td>\n",
              "      <td>business</td>\n",
              "      <td>11124473</td>\n",
              "      <td>87804</td>\n",
              "      <td>266151</td>\n",
              "      <td>25139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>dunkindonuts</td>\n",
              "      <td>America runs on Dunkin‚Äô tweets ‚òïÔ∏èüç©</td>\n",
              "      <td>Dunkin'</td>\n",
              "      <td>True</td>\n",
              "      <td>Canton, Mass.</td>\n",
              "      <td>2007-09-09T20:21:27.000Z</td>\n",
              "      <td>8771022</td>\n",
              "      <td>business</td>\n",
              "      <td>1341144</td>\n",
              "      <td>51036</td>\n",
              "      <td>109341</td>\n",
              "      <td>5494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>netflix</td>\n",
              "      <td>I would watch 100 movies where Nia Long &amp; Laur...</td>\n",
              "      <td>Netflix</td>\n",
              "      <td>True</td>\n",
              "      <td>California, USA</td>\n",
              "      <td>2008-10-03T04:16:17.000Z</td>\n",
              "      <td>16573941</td>\n",
              "      <td>business</td>\n",
              "      <td>21455052</td>\n",
              "      <td>2256</td>\n",
              "      <td>47848</td>\n",
              "      <td>20953</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hulu</td>\n",
              "      <td>Hey Sebastian Stan, we're single this #Valenti...</td>\n",
              "      <td>Hulu</td>\n",
              "      <td>True</td>\n",
              "      <td>üè†</td>\n",
              "      <td>2008-06-07T00:08:57.000Z</td>\n",
              "      <td>15033883</td>\n",
              "      <td>business</td>\n",
              "      <td>1127459</td>\n",
              "      <td>1655</td>\n",
              "      <td>145679</td>\n",
              "      <td>7889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SouthwestAir</td>\n",
              "      <td>We run on #SouthwestHeart. Follow for more on ...</td>\n",
              "      <td>Southwest Airlines</td>\n",
              "      <td>True</td>\n",
              "      <td>Dallas, Texas</td>\n",
              "      <td>2007-07-02T19:56:44.000Z</td>\n",
              "      <td>7212562</td>\n",
              "      <td>business</td>\n",
              "      <td>2170365</td>\n",
              "      <td>0</td>\n",
              "      <td>1100166</td>\n",
              "      <td>13936</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c887a1ad-5e83-4b0b-b81e-dbf7f766906c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c887a1ad-5e83-4b0b-b81e-dbf7f766906c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c887a1ad-5e83-4b0b-b81e-dbf7f766906c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       username                                        description  \\\n",
              "0     Starbucks                           Make today a good day. üíö   \n",
              "1  dunkindonuts                 America runs on Dunkin‚Äô tweets ‚òïÔ∏èüç©   \n",
              "2       netflix  I would watch 100 movies where Nia Long & Laur...   \n",
              "3          hulu  Hey Sebastian Stan, we're single this #Valenti...   \n",
              "4  SouthwestAir  We run on #SouthwestHeart. Follow for more on ...   \n",
              "\n",
              "                 name  verified         location                created_at  \\\n",
              "0    Starbucks Coffee      True      Seattle, WA  2006-11-29T19:19:02.000Z   \n",
              "1             Dunkin'      True    Canton, Mass.  2007-09-09T20:21:27.000Z   \n",
              "2             Netflix      True  California, USA  2008-10-03T04:16:17.000Z   \n",
              "3                Hulu      True                üè†  2008-06-07T00:08:57.000Z   \n",
              "4  Southwest Airlines      True    Dallas, Texas  2007-07-02T19:56:44.000Z   \n",
              "\n",
              "         id verified_type  public_metrics_followers_count  \\\n",
              "0     30973      business                        11124473   \n",
              "1   8771022      business                         1341144   \n",
              "2  16573941      business                        21455052   \n",
              "3  15033883      business                         1127459   \n",
              "4   7212562      business                         2170365   \n",
              "\n",
              "   public_metrics_following_count  public_metrics_tweet_count  \\\n",
              "0                           87804                      266151   \n",
              "1                           51036                      109341   \n",
              "2                            2256                       47848   \n",
              "3                            1655                      145679   \n",
              "4                               0                     1100166   \n",
              "\n",
              "   public_metrics_listed_count  \n",
              "0                        25139  \n",
              "1                         5494  \n",
              "2                        20953  \n",
              "3                         7889  \n",
              "4                        13936  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#create a brand dataframe\n",
        "df_brand = pd.json_normalize(df_brand, sep=\"_\" )\n",
        "df_brand.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "nOZvJwr1A3JV",
        "outputId": "0c6683df-e04e-4768-c1a6-a27a107674a5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-996a4232-3c84-49af-aefe-dbe51028e5ae\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>company_name</th>\n",
              "      <th>brand_id</th>\n",
              "      <th>username</th>\n",
              "      <th>date_joined</th>\n",
              "      <th>bio</th>\n",
              "      <th>location</th>\n",
              "      <th>verified</th>\n",
              "      <th>verified_type</th>\n",
              "      <th>follower_count</th>\n",
              "      <th>following_count</th>\n",
              "      <th>listed_count</th>\n",
              "      <th>tweet_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Starbucks Coffee</td>\n",
              "      <td>30973</td>\n",
              "      <td>Starbucks</td>\n",
              "      <td>2006-11-29T19:19:02.000Z</td>\n",
              "      <td>Make today a good day. üíö</td>\n",
              "      <td>Seattle, WA</td>\n",
              "      <td>True</td>\n",
              "      <td>business</td>\n",
              "      <td>11124473</td>\n",
              "      <td>87804</td>\n",
              "      <td>25139</td>\n",
              "      <td>266151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Dunkin'</td>\n",
              "      <td>8771022</td>\n",
              "      <td>dunkindonuts</td>\n",
              "      <td>2007-09-09T20:21:27.000Z</td>\n",
              "      <td>America runs on Dunkin‚Äô tweets ‚òïÔ∏èüç©</td>\n",
              "      <td>Canton, Mass.</td>\n",
              "      <td>True</td>\n",
              "      <td>business</td>\n",
              "      <td>1341144</td>\n",
              "      <td>51036</td>\n",
              "      <td>5494</td>\n",
              "      <td>109341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Netflix</td>\n",
              "      <td>16573941</td>\n",
              "      <td>netflix</td>\n",
              "      <td>2008-10-03T04:16:17.000Z</td>\n",
              "      <td>I would watch 100 movies where Nia Long &amp; Laur...</td>\n",
              "      <td>California, USA</td>\n",
              "      <td>True</td>\n",
              "      <td>business</td>\n",
              "      <td>21455052</td>\n",
              "      <td>2256</td>\n",
              "      <td>20953</td>\n",
              "      <td>47848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hulu</td>\n",
              "      <td>15033883</td>\n",
              "      <td>hulu</td>\n",
              "      <td>2008-06-07T00:08:57.000Z</td>\n",
              "      <td>Hey Sebastian Stan, we're single this #Valenti...</td>\n",
              "      <td>üè†</td>\n",
              "      <td>True</td>\n",
              "      <td>business</td>\n",
              "      <td>1127459</td>\n",
              "      <td>1655</td>\n",
              "      <td>7889</td>\n",
              "      <td>145679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Southwest Airlines</td>\n",
              "      <td>7212562</td>\n",
              "      <td>SouthwestAir</td>\n",
              "      <td>2007-07-02T19:56:44.000Z</td>\n",
              "      <td>We run on #SouthwestHeart. Follow for more on ...</td>\n",
              "      <td>Dallas, Texas</td>\n",
              "      <td>True</td>\n",
              "      <td>business</td>\n",
              "      <td>2170365</td>\n",
              "      <td>0</td>\n",
              "      <td>13936</td>\n",
              "      <td>1100166</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-996a4232-3c84-49af-aefe-dbe51028e5ae')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-996a4232-3c84-49af-aefe-dbe51028e5ae button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-996a4232-3c84-49af-aefe-dbe51028e5ae');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "         company_name  brand_id      username               date_joined  \\\n",
              "0    Starbucks Coffee     30973     Starbucks  2006-11-29T19:19:02.000Z   \n",
              "1             Dunkin'   8771022  dunkindonuts  2007-09-09T20:21:27.000Z   \n",
              "2             Netflix  16573941       netflix  2008-10-03T04:16:17.000Z   \n",
              "3                Hulu  15033883          hulu  2008-06-07T00:08:57.000Z   \n",
              "4  Southwest Airlines   7212562  SouthwestAir  2007-07-02T19:56:44.000Z   \n",
              "\n",
              "                                                 bio         location  \\\n",
              "0                           Make today a good day. üíö      Seattle, WA   \n",
              "1                 America runs on Dunkin‚Äô tweets ‚òïÔ∏èüç©    Canton, Mass.   \n",
              "2  I would watch 100 movies where Nia Long & Laur...  California, USA   \n",
              "3  Hey Sebastian Stan, we're single this #Valenti...                üè†   \n",
              "4  We run on #SouthwestHeart. Follow for more on ...    Dallas, Texas   \n",
              "\n",
              "   verified verified_type  follower_count  following_count  listed_count  \\\n",
              "0      True      business        11124473            87804         25139   \n",
              "1      True      business         1341144            51036          5494   \n",
              "2      True      business        21455052             2256         20953   \n",
              "3      True      business         1127459             1655          7889   \n",
              "4      True      business         2170365                0         13936   \n",
              "\n",
              "   tweet_count  \n",
              "0       266151  \n",
              "1       109341  \n",
              "2        47848  \n",
              "3       145679  \n",
              "4      1100166  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#rearrange and rename columns\n",
        "df_brand = df_brand.rename(columns={'created_at': 'date_joined', 'description': 'bio', 'id': 'brand_id', 'name': 'company_name',\n",
        "                                    'public_metrics_followers_count': 'follower_count', 'public_metrics_following_count': 'following_count',\n",
        "                                    'public_metrics_listed_count': 'listed_count', 'public_metrics_tweet_count': 'tweet_count'})\n",
        "\n",
        "df_brand = df_brand[['company_name', 'brand_id', 'username', 'date_joined', 'bio', 'location', 'verified',\n",
        "                     'verified_type','follower_count','following_count','listed_count','tweet_count']]\n",
        "df_brand.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "OBOii2ZhwyNf",
        "outputId": "475e807b-6404-4484-e9f8-296a94d52498"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_49a7bba8-11ff-4428-88b8-b9c432c4927e\", \"Brand.csv\", 1228)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#download the brand dataframe as a .csv file\n",
        "df_brand.to_csv('Brand.csv', index=False)\n",
        "files.download('Brand.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0eFO5zjohFP"
      },
      "source": [
        "## Tweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YArG3FZ5EnI3"
      },
      "source": [
        "#### Starbucks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uu8NMTKto-34"
      },
      "outputs": [],
      "source": [
        "# Create an empty Pandas DataFrame\n",
        "starbucks_tweets = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQA8RVMtjNQP"
      },
      "outputs": [],
      "source": [
        "#search tweets by page limit\n",
        "page_search = tweepy.Paginator(\n",
        "    method = client.search_recent_tweets,\n",
        "    limit=10,\n",
        "    query=\"Starbucks lang:en -is:retweet\",   #pulls all tweets that mention Dunkin and are in English, excludes retweets\n",
        "    expansions = [\"author_id\"],\n",
        "\n",
        "    ###change these dates to required time range###\n",
        "    start_time = \"2023-03-12T00:00:00-05:00\",\n",
        "    end_time = \"2023-03-12T23:59:59-05:00\",\n",
        "    ###############################################\n",
        "\n",
        "    tweet_fields = [\"created_at\", \"id\",\"public_metrics\", \"text\"], #information to be store from each tweet\n",
        "    max_results=100  #limits the number of tweets pulled\n",
        "    )\n",
        "\n",
        "# Iterate through each page of results\n",
        "for n,page in enumerate(page_search):\n",
        "\n",
        "  # Print the current page number\n",
        "  #print(\"PAGE NUMBER: \",n)\n",
        "\n",
        "  # Iterate through each tweet in the page.data, convert to a dataframe, and\n",
        "\n",
        "  for tweet in page.data:\n",
        "    temp_df = pd.json_normalize(tweet.data, sep=\"_\")\n",
        "    starbucks_tweets = starbucks_tweets.append(temp_df, ignore_index=True)  # append to df_tweets\n",
        "\n",
        "\n",
        "###############################################\n",
        "###############################################\n",
        "\n",
        "#search tweets by page limit\n",
        "page_search = tweepy.Paginator(\n",
        "    method = client.search_recent_tweets,\n",
        "    limit=10,\n",
        "    query=\"Starbucks lang:en -is:retweet\",   #pulls all tweets that mention Dunkin and are in English, excludes retweets\n",
        "    expansions = [\"author_id\"],\n",
        "\n",
        "    ###change these dates to required time range###\n",
        "    start_time = \"2023-03-07T00:00:00-05:00\",\n",
        "    end_time = \"2023-03-07T23:59:59-05:00\",\n",
        "    ###############################################\n",
        "\n",
        "    tweet_fields = [\"created_at\", \"id\",\"public_metrics\", \"text\"], #information to be store from each tweet\n",
        "    max_results=100  #limits the number of tweets pulled\n",
        "    )\n",
        "\n",
        "# Iterate through each page of results\n",
        "for n,page in enumerate(page_search):\n",
        "\n",
        "  # Print the current page number\n",
        "  # print(\"PAGE NUMBER: \",n)\n",
        "\n",
        "  # Iterate through each tweet in the page.data, convert to a dataframe, and\n",
        "\n",
        "  for tweet in page.data:\n",
        "    temp_df = pd.json_normalize(tweet.data, sep=\"_\")\n",
        "    starbucks_tweets = starbucks_tweets.append(temp_df, ignore_index=True)  # append to df_tweets\n",
        "\n",
        "###############################################\n",
        "###############################################\n",
        "\n",
        "\n",
        "#search tweets by page limit\n",
        "page_search = tweepy.Paginator(\n",
        "    method = client.search_recent_tweets,\n",
        "    limit=10,\n",
        "    query=\"Starbucks lang:en -is:retweet\",   #pulls all tweets that mention Dunkin and are in English, excludes retweets\n",
        "    expansions = [\"author_id\"],\n",
        "\n",
        "    ###change these dates to required time range###\n",
        "    start_time = \"2023-03-08T00:00:00-05:00\",\n",
        "    end_time = \"2023-03-08T23:59:59-05:00\",\n",
        "    ###############################################\n",
        "\n",
        "    tweet_fields = [\"created_at\", \"id\",\"public_metrics\", \"text\"], #information to be store from each tweet\n",
        "    max_results=100  #limits the number of tweets pulled\n",
        "    )\n",
        "\n",
        "# Iterate through each page of results\n",
        "for n,page in enumerate(page_search):\n",
        "\n",
        "  # Print the current page number\n",
        "  #print(\"PAGE NUMBER: \",n)\n",
        "\n",
        "  # Iterate through each tweet in the page.data, convert to a dataframe, and\n",
        "\n",
        "  for tweet in page.data:\n",
        "    temp_df = pd.json_normalize(tweet.data, sep=\"_\")\n",
        "    starbucks_tweets = starbucks_tweets.append(temp_df, ignore_index=True)  # append to df_tweets\n",
        "\n",
        "\n",
        "###############################################\n",
        "###############################################\n",
        "\n",
        "\n",
        "#search tweets by page limit\n",
        "page_search = tweepy.Paginator(\n",
        "    method = client.search_recent_tweets,\n",
        "    limit=10,\n",
        "    query=\"Starbucks lang:en -is:retweet\",   #pulls all tweets that mention Dunkin and are in English, excludes retweets\n",
        "    expansions = [\"author_id\"],\n",
        "\n",
        "    ###change these dates to required time range###\n",
        "    start_time = \"2023-03-09T00:00:00-05:00\",\n",
        "    end_time = \"2023-03-09T23:59:59-05:00\",\n",
        "    ###############################################\n",
        "\n",
        "    tweet_fields = [\"created_at\", \"id\",\"public_metrics\", \"text\"], #information to be store from each tweet\n",
        "    max_results=100  #limits the number of tweets pulled\n",
        "    )\n",
        "\n",
        "# Iterate through each page of results\n",
        "for n,page in enumerate(page_search):\n",
        "\n",
        "  # Print the current page number\n",
        "  #print(\"PAGE NUMBER: \",n)\n",
        "\n",
        "  # Iterate through each tweet in the page.data, convert to a dataframe, and\n",
        "\n",
        "  for tweet in page.data:\n",
        "    temp_df = pd.json_normalize(tweet.data, sep=\"_\")\n",
        "    starbucks_tweets = starbucks_tweets.append(temp_df, ignore_index=True)  # append to df_tweets\n",
        "\n",
        "\n",
        "###############################################\n",
        "###############################################\n",
        "\n",
        "\n",
        "#search tweets by page limit\n",
        "page_search = tweepy.Paginator(\n",
        "    method = client.search_recent_tweets,\n",
        "    limit=10,\n",
        "    query=\"Starbucks lang:en -is:retweet\",   #pulls all tweets that mention Dunkin and are in English, excludes retweets\n",
        "    expansions = [\"author_id\"],\n",
        "\n",
        "    ###change these dates to required time range###\n",
        "    start_time = \"2023-03-10T00:00:00-05:00\",\n",
        "    end_time = \"2023-03-10T23:59:59-05:00\",\n",
        "    ###############################################\n",
        "\n",
        "    tweet_fields = [\"created_at\", \"id\",\"public_metrics\", \"text\"], #information to be store from each tweet\n",
        "    max_results=100  #limits the number of tweets pulled\n",
        "    )\n",
        "\n",
        "# Iterate through each page of results\n",
        "for n,page in enumerate(page_search):\n",
        "\n",
        "  # Print the current page number\n",
        "  #print(\"PAGE NUMBER: \",n)\n",
        "\n",
        "  # Iterate through each tweet in the page.data, convert to a dataframe, and\n",
        "\n",
        "  for tweet in page.data:\n",
        "    temp_df = pd.json_normalize(tweet.data, sep=\"_\")\n",
        "    starbucks_tweets = starbucks_tweets.append(temp_df, ignore_index=True)  # append to df_tweets\n",
        "\n",
        "\n",
        "    ###############################################\n",
        "###############################################\n",
        "\n",
        "\n",
        "#search tweets by page limit\n",
        "page_search = tweepy.Paginator(\n",
        "    method = client.search_recent_tweets,\n",
        "    limit=10,\n",
        "    query=\"Starbucks lang:en -is:retweet\",   #pulls all tweets that mention Dunkin and are in English, excludes retweets\n",
        "    expansions = [\"author_id\"],\n",
        "\n",
        "    ###change these dates to required time range###\n",
        "    start_time = \"2023-03-11T00:00:00-05:00\",\n",
        "    end_time = \"2023-03-11T23:59:59-05:00\",\n",
        "    ###############################################\n",
        "\n",
        "    tweet_fields = [\"created_at\", \"id\",\"public_metrics\", \"text\"], #information to be store from each tweet\n",
        "    max_results=100  #limits the number of tweets pulled\n",
        "    )\n",
        "\n",
        "# Iterate through each page of results\n",
        "for n,page in enumerate(page_search):\n",
        "\n",
        "  # Print the current page number\n",
        "  #print(\"PAGE NUMBER: \",n)\n",
        "\n",
        "  # Iterate through each tweet in the page.data, convert to a dataframe, and\n",
        "\n",
        "  for tweet in page.data:\n",
        "    temp_df = pd.json_normalize(tweet.data, sep=\"_\")\n",
        "    starbucks_tweets = starbucks_tweets.append(temp_df, ignore_index=True)  # append to df_tweets\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#     ###############################################\n",
        "# ###############################################\n",
        "\n",
        "\n",
        "# #search tweets by page limit\n",
        "# page_search = tweepy.Paginator(\n",
        "#     method = client.search_recent_tweets,\n",
        "#     limit=10,\n",
        "#     query=\"Starbucks lang:en -is:retweet\",   #pulls all tweets that mention Dunkin and are in English, excludes retweets\n",
        "#     expansions = [\"author_id\"],\n",
        "\n",
        "#     ###change these dates to required time range###\n",
        "#     start_time = \"2023-02-27T00:00:00-05:00\",\n",
        "#     end_time = \"2023-02-27T23:59:59-05:00\",\n",
        "#     ###############################################\n",
        "\n",
        "#     tweet_fields = [\"created_at\", \"id\",\"public_metrics\", \"text\"], #information to be store from each tweet\n",
        "#     max_results=100  #limits the number of tweets pulled\n",
        "#     )\n",
        "\n",
        "# # Iterate through each page of results\n",
        "# for n,page in enumerate(page_search):\n",
        "\n",
        "#   # Print the current page number\n",
        "#   #print(\"PAGE NUMBER: \",n)\n",
        "\n",
        "#   # Iterate through each tweet in the page.data, convert to a dataframe, and\n",
        "\n",
        "#   for tweet in page.data:\n",
        "#     temp_df = pd.json_normalize(tweet.data, sep=\"_\")\n",
        "#     starbucks_tweets = starbucks_tweets.append(temp_df, ignore_index=True)  # append to df_tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "C-JuI7L4jNWQ",
        "outputId": "cde39637-6218-4c1a-bd64-f27e0ffa91aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              tweet_id              user_id               date_posted  \\\n",
              "0  1635143549179031557  1470638974964277249  2023-03-13T04:59:36.000Z   \n",
              "1  1635143531944873985  1519054874067021831  2023-03-13T04:59:32.000Z   \n",
              "2  1635143475749613569  1519054874067021831  2023-03-13T04:59:18.000Z   \n",
              "3  1635143416932872193             80872391  2023-03-13T04:59:04.000Z   \n",
              "4  1635143391284727809  1599035809763848194  2023-03-13T04:58:58.000Z   \n",
              "\n",
              "   retweet_count  reply_count  like_count  quotetweet_count  impression_count  \\\n",
              "0              0            0           0                 0                 4   \n",
              "1              0            0           0                 0                21   \n",
              "2              0            1           0                 0                28   \n",
              "3              0            0           0                 0                15   \n",
              "4              0            0           0                 0                 3   \n",
              "\n",
              "                                                text  \n",
              "0  @CalltoActivism @Starbucks Could be Trumps sis...  \n",
              "1  @CidRising @VVh1sp3r @CalltoActivism @Starbuck...  \n",
              "2  @VVh1sp3r @CidRising @CalltoActivism @Starbuck...  \n",
              "3  @khatumokeen your people are wandering in the ...  \n",
              "4  @CalltoActivism @Starbucks I‚Äôm having a hard t...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e885f946-9283-4797-8be8-8abbd1335f2c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>user_id</th>\n",
              "      <th>date_posted</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>reply_count</th>\n",
              "      <th>like_count</th>\n",
              "      <th>quotetweet_count</th>\n",
              "      <th>impression_count</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1635143549179031557</td>\n",
              "      <td>1470638974964277249</td>\n",
              "      <td>2023-03-13T04:59:36.000Z</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>@CalltoActivism @Starbucks Could be Trumps sis...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1635143531944873985</td>\n",
              "      <td>1519054874067021831</td>\n",
              "      <td>2023-03-13T04:59:32.000Z</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>21</td>\n",
              "      <td>@CidRising @VVh1sp3r @CalltoActivism @Starbuck...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1635143475749613569</td>\n",
              "      <td>1519054874067021831</td>\n",
              "      <td>2023-03-13T04:59:18.000Z</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>28</td>\n",
              "      <td>@VVh1sp3r @CidRising @CalltoActivism @Starbuck...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1635143416932872193</td>\n",
              "      <td>80872391</td>\n",
              "      <td>2023-03-13T04:59:04.000Z</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>@khatumokeen your people are wandering in the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1635143391284727809</td>\n",
              "      <td>1599035809763848194</td>\n",
              "      <td>2023-03-13T04:58:58.000Z</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>@CalltoActivism @Starbucks I‚Äôm having a hard t...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e885f946-9283-4797-8be8-8abbd1335f2c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e885f946-9283-4797-8be8-8abbd1335f2c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e885f946-9283-4797-8be8-8abbd1335f2c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "#drop, rearrange, and rename columns\n",
        "starbucks_tweets.drop(columns = 'edit_history_tweet_ids')\n",
        "starbucks_tweets = starbucks_tweets.rename(columns={'id': 'tweet_id', 'author_id': 'user_id', 'created_at': 'date_posted',\n",
        "                                    'public_metrics_retweet_count': 'retweet_count', 'public_metrics_reply_count': 'reply_count',\n",
        "                                    'public_metrics_like_count': 'like_count', 'public_metrics_quote_count': 'quotetweet_count',\n",
        "                                    'public_metrics_impression_count':'impression_count'})\n",
        "\n",
        "starbucks_tweets = starbucks_tweets[['tweet_id', 'user_id', 'date_posted','retweet_count', 'reply_count', 'like_count', 'quotetweet_count',\n",
        "                     'impression_count','text']]\n",
        "starbucks_tweets.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyjYsVTYvmA8",
        "outputId": "a00f84f9-9a64-4d8a-8e30-ad79e4fba6c2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5867, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "starbucks_tweets.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "2Co-7tdWe0F2",
        "outputId": "805e3bc6-45c4-4c6e-d867-b19d4f8de7f1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b8d4ef3b-578c-43c7-830a-10b127f53711\", \"Starbucks.csv\", 1161050)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#download the user dataframe as a .csv file\n",
        "starbucks_tweets.to_csv('Starbucks.csv', index=False)\n",
        "files.download('Starbucks.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUyfUKQLifK8"
      },
      "source": [
        "### Dunkin'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vtDEeRDmWLi"
      },
      "outputs": [],
      "source": [
        "# Create an empty Pandas DataFrame\n",
        "dunkin_tweets = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3YLyOlBijRG"
      },
      "outputs": [],
      "source": [
        "#search tweets by page limit\n",
        "page_search = tweepy.Paginator(\n",
        "    method = client.search_recent_tweets,\n",
        "    limit=10,\n",
        "    query=\"(Dunkin OR Dunkin') lang:en -is:retweet\",   #pulls all tweets that mention Dunkin and are in English, excludes retweets\n",
        "    expansions = [\"author_id\"],\n",
        "\n",
        "    ###change these dates to required time range###\n",
        "    start_time = \"2023-03-07T00:00:00-05:00\",\n",
        "    end_time = \"2023-03-07T23:59:59-05:00\",\n",
        "    ###############################################\n",
        "\n",
        "    tweet_fields = [\"created_at\", \"id\",\"public_metrics\", \"text\"], #information to be store from each tweet\n",
        "    max_results=100  #limits the number of tweets pulled\n",
        "    )\n",
        "\n",
        "# Iterate through each page of results\n",
        "for n,page in enumerate(page_search):\n",
        "\n",
        "  # Print the current page number\n",
        "  print(\"PAGE NUMBER: \",n)\n",
        "\n",
        "  # Iterate through each tweet in the page.data, convert to a dataframe, and\n",
        "\n",
        "  for tweet in page.data:\n",
        "    temp_df = pd.json_normalize(tweet.data, sep=\"_\")\n",
        "    dunkin_tweets = dunkin_tweets.append(temp_df, ignore_index=True)  # append to df_tweets\n",
        "\n",
        "\n",
        "###############################################\n",
        "###############################################\n",
        "\n",
        "#search tweets by page limit\n",
        "page_search = tweepy.Paginator(\n",
        "    method = client.search_recent_tweets,\n",
        "    limit=10,\n",
        "    query=\"(Dunkin OR Dunkin') lang:en -is:retweet\",   #pulls all tweets that mention Dunkin and are in English, excludes retweets\n",
        "    expansions = [\"author_id\"],\n",
        "\n",
        "    ###change these dates to required time range###\n",
        "    start_time = \"2023-03-08T00:00:00-05:00\",\n",
        "    end_time = \"2023-03-08T23:59:59-05:00\",\n",
        "    ###############################################\n",
        "\n",
        "    tweet_fields = [\"created_at\", \"id\",\"public_metrics\", \"text\"], #information to be store from each tweet\n",
        "    max_results=100  #limits the number of tweets pulled\n",
        "    )\n",
        "\n",
        "# Iterate through each page of results\n",
        "for n,page in enumerate(page_search):\n",
        "\n",
        "  # Print the current page number\n",
        "  # print(\"PAGE NUMBER: \",n)\n",
        "\n",
        "  # Iterate through each tweet in the page.data, convert to a dataframe, and\n",
        "\n",
        "  for tweet in page.data:\n",
        "    temp_df = pd.json_normalize(tweet.data, sep=\"_\")\n",
        "    dunkin_tweets = dunkin_tweets.append(temp_df, ignore_index=True)  # append to df_tweets\n",
        "\n",
        "###############################################\n",
        "###############################################\n",
        "\n",
        "\n",
        "#search tweets by page limit\n",
        "page_search = tweepy.Paginator(\n",
        "    method = client.search_recent_tweets,\n",
        "    limit=10,\n",
        "    query=\"(Dunkin OR Dunkin') lang:en -is:retweet\",   #pulls all tweets that mention Dunkin and are in English, excludes retweets\n",
        "    expansions = [\"author_id\"],\n",
        "\n",
        "    ###change these dates to required time range###\n",
        "    start_time = \"2023-03-09T00:00:00-05:00\",\n",
        "    end_time = \"2023-03-09T23:59:59-05:00\",\n",
        "    ###############################################\n",
        "\n",
        "    tweet_fields = [\"created_at\", \"id\",\"public_metrics\", \"text\"], #information to be store from each tweet\n",
        "    max_results=100  #limits the number of tweets pulled\n",
        "    )\n",
        "\n",
        "# Iterate through each page of results\n",
        "for n,page in enumerate(page_search):\n",
        "\n",
        "  # Print the current page number\n",
        "  print(\"PAGE NUMBER: \",n)\n",
        "\n",
        "  # Iterate through each tweet in the page.data, convert to a dataframe, and\n",
        "\n",
        "  for tweet in page.data:\n",
        "    temp_df = pd.json_normalize(tweet.data, sep=\"_\")\n",
        "    dunkin_tweets = dunkin_tweets.append(temp_df, ignore_index=True)  # append to df_tweets\n",
        "\n",
        "\n",
        "###############################################\n",
        "###############################################\n",
        "\n",
        "\n",
        "#search tweets by page limit\n",
        "page_search = tweepy.Paginator(\n",
        "    method = client.search_recent_tweets,\n",
        "    limit=10,\n",
        "    query=\"(Dunkin OR Dunkin') lang:en -is:retweet\",   #pulls all tweets that mention Dunkin and are in English, excludes retweets\n",
        "    expansions = [\"author_id\"],\n",
        "\n",
        "    ###change these dates to required time range###\n",
        "    start_time = \"2023-03-10T00:00:00-05:00\",\n",
        "    end_time = \"2023-03-10T23:59:59-05:00\",\n",
        "    ###############################################\n",
        "\n",
        "    tweet_fields = [\"created_at\", \"id\",\"public_metrics\", \"text\"], #information to be store from each tweet\n",
        "    max_results=100  #limits the number of tweets pulled\n",
        "    )\n",
        "\n",
        "# Iterate through each page of results\n",
        "for n,page in enumerate(page_search):\n",
        "\n",
        "  # Print the current page number\n",
        "  print(\"PAGE NUMBER: \",n)\n",
        "\n",
        "  # Iterate through each tweet in the page.data, convert to a dataframe, and\n",
        "\n",
        "  for tweet in page.data:\n",
        "    temp_df = pd.json_normalize(tweet.data, sep=\"_\")\n",
        "    dunkin_tweets = dunkin_tweets.append(temp_df, ignore_index=True)  # append to df_tweets\n",
        "\n",
        "\n",
        "###############################################\n",
        "###############################################\n",
        "\n",
        "\n",
        "#search tweets by page limit\n",
        "page_search = tweepy.Paginator(\n",
        "    method = client.search_recent_tweets,\n",
        "    limit=10,\n",
        "    query=\"(Dunkin OR Dunkin') lang:en -is:retweet\",   #pulls all tweets that mention Dunkin and are in English, excludes retweets\n",
        "    expansions = [\"author_id\"],\n",
        "\n",
        "    ###change these dates to required time range###\n",
        "    start_time = \"2023-03-11T00:00:00-05:00\",\n",
        "    end_time = \"2023-03-11T23:59:59-05:00\",\n",
        "    ###############################################\n",
        "\n",
        "    tweet_fields = [\"created_at\", \"id\",\"public_metrics\", \"text\"], #information to be store from each tweet\n",
        "    max_results=100  #limits the number of tweets pulled\n",
        "    )\n",
        "\n",
        "# Iterate through each page of results\n",
        "for n,page in enumerate(page_search):\n",
        "\n",
        "  # Print the current page number\n",
        "  print(\"PAGE NUMBER: \",n)\n",
        "\n",
        "  # Iterate through each tweet in the page.data, convert to a dataframe, and\n",
        "\n",
        "  for tweet in page.data:\n",
        "    temp_df = pd.json_normalize(tweet.data, sep=\"_\")\n",
        "    dunkin_tweets = dunkin_tweets.append(temp_df, ignore_index=True)  # append to df_tweets\n",
        "\n",
        "\n",
        "###############################################\n",
        "###############################################\n",
        "\n",
        "\n",
        "#search tweets by page limit\n",
        "page_search = tweepy.Paginator(\n",
        "    method = client.search_recent_tweets,\n",
        "    limit=10,\n",
        "    query=\"(Dunkin OR Dunkin') lang:en -is:retweet\",   #pulls all tweets that mention Dunkin and are in English, excludes retweets\n",
        "    expansions = [\"author_id\"],\n",
        "\n",
        "    ###change these dates to required time range###\n",
        "    start_time = \"2023-03-12T00:00:00-05:00\",\n",
        "    end_time = \"2023-03-12T23:59:59-05:00\",\n",
        "    ###############################################\n",
        "\n",
        "    tweet_fields = [\"created_at\", \"id\",\"public_metrics\", \"text\"], #information to be store from each tweet\n",
        "    max_results=100  #limits the number of tweets pulled\n",
        "    )\n",
        "\n",
        "# Iterate through each page of results\n",
        "for n,page in enumerate(page_search):\n",
        "\n",
        "  # Print the current page number\n",
        "  print(\"PAGE NUMBER: \",n)\n",
        "\n",
        "  # Iterate through each tweet in the page.data, convert to a dataframe, and\n",
        "\n",
        "  for tweet in page.data:\n",
        "    temp_df = pd.json_normalize(tweet.data, sep=\"_\")\n",
        "    dunkin_tweets = dunkin_tweets.append(temp_df, ignore_index=True)  # append to df_tweets\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "Gq7fHvzTmOoW",
        "outputId": "c1a0e7bf-67b1-4069-952c-6748b01c50c0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              tweet_id              user_id               date_posted  \\\n",
              "0  1633331644621901825  1454673423167369216  2023-03-08T04:59:44.000Z   \n",
              "1  1633331130312040448           1909828159  2023-03-08T04:57:41.000Z   \n",
              "2  1633330971989667841            147230484  2023-03-08T04:57:04.000Z   \n",
              "3  1633330968072425472  1633323380257755138  2023-03-08T04:57:03.000Z   \n",
              "4  1633330793702477826           3741378973  2023-03-08T04:56:21.000Z   \n",
              "\n",
              "   retweet_count  reply_count  like_count  quotetweet_count  impression_count  \\\n",
              "0              0            1           5                 0               195   \n",
              "1              0            0           0                 0                50   \n",
              "2              0            0           0                 0                 2   \n",
              "3              0            0           1                 0                70   \n",
              "4              0            0           1                 0                67   \n",
              "\n",
              "                                                text  \n",
              "0  People want to love themselves and conquer oth...  \n",
              "1                   I think I‚Äôm back on Dunkin bad üò≠  \n",
              "2  Play the Dunkin' Cart Catch game for chances t...  \n",
              "3  POV dunkin on the haters üòúüòú https://t.co/rYave...  \n",
              "4                             @kingdee_11 Dunkin lol  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-63fa6484-2c1b-4f13-a14b-e98cf80bcf5d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>user_id</th>\n",
              "      <th>date_posted</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>reply_count</th>\n",
              "      <th>like_count</th>\n",
              "      <th>quotetweet_count</th>\n",
              "      <th>impression_count</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1633331644621901825</td>\n",
              "      <td>1454673423167369216</td>\n",
              "      <td>2023-03-08T04:59:44.000Z</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>195</td>\n",
              "      <td>People want to love themselves and conquer oth...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1633331130312040448</td>\n",
              "      <td>1909828159</td>\n",
              "      <td>2023-03-08T04:57:41.000Z</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>50</td>\n",
              "      <td>I think I‚Äôm back on Dunkin bad üò≠</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1633330971989667841</td>\n",
              "      <td>147230484</td>\n",
              "      <td>2023-03-08T04:57:04.000Z</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>Play the Dunkin' Cart Catch game for chances t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1633330968072425472</td>\n",
              "      <td>1633323380257755138</td>\n",
              "      <td>2023-03-08T04:57:03.000Z</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>70</td>\n",
              "      <td>POV dunkin on the haters üòúüòú https://t.co/rYave...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1633330793702477826</td>\n",
              "      <td>3741378973</td>\n",
              "      <td>2023-03-08T04:56:21.000Z</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>67</td>\n",
              "      <td>@kingdee_11 Dunkin lol</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-63fa6484-2c1b-4f13-a14b-e98cf80bcf5d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-63fa6484-2c1b-4f13-a14b-e98cf80bcf5d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-63fa6484-2c1b-4f13-a14b-e98cf80bcf5d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "#drop, rearrange, and rename columns\n",
        "dunkin_tweets.drop(columns = 'edit_history_tweet_ids')\n",
        "dunkin_tweets = dunkin_tweets.rename(columns={'id': 'tweet_id', 'author_id': 'user_id', 'created_at': 'date_posted',\n",
        "                                    'public_metrics_retweet_count': 'retweet_count', 'public_metrics_reply_count': 'reply_count',\n",
        "                                    'public_metrics_like_count': 'like_count', 'public_metrics_quote_count': 'quotetweet_count',\n",
        "                                    'public_metrics_impression_count':'impression_count'})\n",
        "\n",
        "dunkin_tweets = dunkin_tweets[['tweet_id', 'user_id', 'date_posted','retweet_count', 'reply_count', 'like_count', 'quotetweet_count',\n",
        "                     'impression_count','text']]\n",
        "dunkin_tweets.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4v3Qb3BDijZO",
        "outputId": "b6ead92c-2609-4a72-9013-17a1c53bd308"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5869, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "dunkin_tweets.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "qPq48oZjl9Wn",
        "outputId": "286bd7b5-21b6-428f-cc96-6f66aed80b88"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f6770863-f772-48c4-8c55-9ae1f7c66478\", \"Dunkin.csv\", 1110143)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#download the user dataframe as a .csv file\n",
        "dunkin_tweets.to_csv('Dunkin.csv', index=False)\n",
        "files.download('Dunkin.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XGpZACgifT_"
      },
      "source": [
        "###Netflix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKyVB376puXx"
      },
      "outputs": [],
      "source": [
        "# Create an empty Pandas DataFrame\n",
        "netflix_tweets = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgDir4VfpwWM"
      },
      "outputs": [],
      "source": [
        "#search tweets by page limit\n",
        "page_search = tweepy.Paginator(\n",
        "    method = client.search_recent_tweets,\n",
        "    limit=10,\n",
        "    query=\"Netflix lang:en -is:retweet\",   #pulls all tweets that mention Dunkin and are in English, excludes retweets\n",
        "    expansions = [\"author_id\"],\n",
        "\n",
        "    ###change these dates to required time range###\n",
        "    start_time = \"2023-03-07T00:00:00-05:00\",\n",
        "    end_time = \"2023-03-07T23:59:59-05:00\",\n",
        "    ###############################################\n",
        "\n",
        "    tweet_fields = [\"created_at\", \"id\",\"public_metrics\", \"text\"], #information to be store from each tweet\n",
        "    max_results=100  #limits the number of tweets pulled\n",
        "    )\n",
        "\n",
        "# Iterate through each page of results\n",
        "for n,page in enumerate(page_search):\n",
        "\n",
        "  # Print the current page number\n",
        "  #print(\"PAGE NUMBER: \",n)\n",
        "\n",
        "  # Iterate through each tweet in the page.data, convert to a dataframe, and\n",
        "\n",
        "  for tweet in page.data:\n",
        "    temp_df = pd.json_normalize(tweet.data, sep=\"_\")\n",
        "    netflix_tweets = netflix_tweets.append(temp_df, ignore_index=True)  # append to df_tweets\n",
        "\n",
        "\n",
        "###############################################\n",
        "###############################################\n",
        "\n",
        "#search tweets by page limit\n",
        "page_search = tweepy.Paginator(\n",
        "    method = client.search_recent_tweets,\n",
        "    limit=10,\n",
        "    query=\"Netflix lang:en -is:retweet\",   #pulls all tweets that mention Dunkin and are in English, excludes retweets\n",
        "    expansions = [\"author_id\"],\n",
        "\n",
        "    ###change these dates to required time range###\n",
        "    start_time = \"2023-03-08T00:00:00-05:00\",\n",
        "    end_time = \"2023-03-08T23:59:59-05:00\",\n",
        "    ###############################################\n",
        "\n",
        "    tweet_fields = [\"created_at\", \"id\",\"public_metrics\", \"text\"], #information to be store from each tweet\n",
        "    max_results=100  #limits the number of tweets pulled\n",
        "    )\n",
        "\n",
        "# Iterate through each page of results\n",
        "for n,page in enumerate(page_search):\n",
        "\n",
        "  # Print the current page number\n",
        "  # print(\"PAGE NUMBER: \",n)\n",
        "\n",
        "  # Iterate through each tweet in the page.data, convert to a dataframe, and\n",
        "\n",
        "  for tweet in page.data:\n",
        "    temp_df = pd.json_normalize(tweet.data, sep=\"_\")\n",
        "    netflix_tweets = netflix_tweets.append(temp_df, ignore_index=True)  # append to df_tweets\n",
        "\n",
        "###############################################\n",
        "###############################################\n",
        "\n",
        "\n",
        "#search tweets by page limit\n",
        "page_search = tweepy.Paginator(\n",
        "    method = client.search_recent_tweets,\n",
        "    limit=10,\n",
        "    query=\"Netflix lang:en -is:retweet\",   #pulls all tweets that mention Dunkin and are in English, excludes retweets\n",
        "    expansions = [\"author_id\"],\n",
        "\n",
        "    ###change these dates to required time range###\n",
        "    start_time = \"2023-03-09T00:00:00-05:00\",\n",
        "    end_time = \"2023-03-09T23:59:59-05:00\",\n",
        "    ###############################################\n",
        "\n",
        "    tweet_fields = [\"created_at\", \"id\",\"public_metrics\", \"text\"], #information to be store from each tweet\n",
        "    max_results=100  #limits the number of tweets pulled\n",
        "    )\n",
        "\n",
        "# Iterate through each page of results\n",
        "for n,page in enumerate(page_search):\n",
        "\n",
        "  # Print the current page number\n",
        "  #print(\"PAGE NUMBER: \",n)\n",
        "\n",
        "  # Iterate through each tweet in the page.data, convert to a dataframe, and\n",
        "\n",
        "  for tweet in page.data:\n",
        "    temp_df = pd.json_normalize(tweet.data, sep=\"_\")\n",
        "    netflix_tweets = netflix_tweets.append(temp_df, ignore_index=True)  # append to df_tweets\n",
        "\n",
        "\n",
        "###############################################\n",
        "###############################################\n",
        "\n",
        "\n",
        "#search tweets by page limit\n",
        "page_search = tweepy.Paginator(\n",
        "    method = client.search_recent_tweets,\n",
        "    limit=10,\n",
        "    query=\"Netflix lang:en -is:retweet\",   #pulls all tweets that mention Dunkin and are in English, excludes retweets\n",
        "    expansions = [\"author_id\"],\n",
        "\n",
        "    ###change these dates to required time range###\n",
        "    start_time = \"2023-03-10T00:00:00-05:00\",\n",
        "    end_time = \"2023-03-10T23:59:59-05:00\",\n",
        "    ###############################################\n",
        "\n",
        "    tweet_fields = [\"created_at\", \"id\",\"public_metrics\", \"text\"], #information to be store from each tweet\n",
        "    max_results=100  #limits the number of tweets pulled\n",
        "    )\n",
        "\n",
        "# Iterate through each page of results\n",
        "for n,page in enumerate(page_search):\n",
        "\n",
        "  # Print the current page number\n",
        "  #print(\"PAGE NUMBER: \",n)\n",
        "\n",
        "  # Iterate through each tweet in the page.data, convert to a dataframe, and\n",
        "\n",
        "  for tweet in page.data:\n",
        "    temp_df = pd.json_normalize(tweet.data, sep=\"_\")\n",
        "    netflix_tweets = netflix_tweets.append(temp_df, ignore_index=True)  # append to df_tweets\n",
        "\n",
        "\n",
        "###############################################\n",
        "###############################################\n",
        "\n",
        "\n",
        "#search tweets by page limit\n",
        "page_search = tweepy.Paginator(\n",
        "    method = client.search_recent_tweets,\n",
        "    limit=10,\n",
        "    query=\"Netflix lang:en -is:retweet\",   #pulls all tweets that mention Dunkin and are in English, excludes retweets\n",
        "    expansions = [\"author_id\"],\n",
        "\n",
        "    ###change these dates to required time range###\n",
        "    start_time = \"2023-03-11T00:00:00-05:00\",\n",
        "    end_time = \"2023-03-11T23:59:59-05:00\",\n",
        "    ###############################################\n",
        "\n",
        "    tweet_fields = [\"created_at\", \"id\",\"public_metrics\", \"text\"], #information to be store from each tweet\n",
        "    max_results=100  #limits the number of tweets pulled\n",
        "    )\n",
        "\n",
        "# Iterate through each page of results\n",
        "for n,page in enumerate(page_search):\n",
        "\n",
        "  # Print the current page number\n",
        "  #print(\"PAGE NUMBER: \",n)\n",
        "\n",
        "  # Iterate through each tweet in the page.data, convert to a dataframe, and\n",
        "\n",
        "  for tweet in page.data:\n",
        "    temp_df = pd.json_normalize(tweet.data, sep=\"_\")\n",
        "    netflix_tweets = netflix_tweets.append(temp_df, ignore_index=True)  # append to df_tweets\n",
        "\n",
        "\n",
        "###############################################\n",
        "###############################################\n",
        "\n",
        "\n",
        "#search tweets by page limit\n",
        "page_search = tweepy.Paginator(\n",
        "    method = client.search_recent_tweets,\n",
        "    limit=10,\n",
        "    query=\"Netflix lang:en -is:retweet\",   #pulls all tweets that mention Dunkin and are in English, excludes retweets\n",
        "    expansions = [\"author_id\"],\n",
        "\n",
        "    ###change these dates to required time range###\n",
        "    start_time = \"2023-03-12T00:00:00-05:00\",\n",
        "    end_time = \"2023-03-12T23:59:59-05:00\",\n",
        "    ###############################################\n",
        "\n",
        "    tweet_fields = [\"created_at\", \"id\",\"public_metrics\", \"text\"], #information to be store from each tweet\n",
        "    max_results=100  #limits the number of tweets pulled\n",
        "    )\n",
        "\n",
        "# Iterate through each page of results\n",
        "for n,page in enumerate(page_search):\n",
        "\n",
        "  # Print the current page number\n",
        "  #print(\"PAGE NUMBER: \",n)\n",
        "\n",
        "  # Iterate through each tweet in the page.data, convert to a dataframe, and\n",
        "\n",
        "  for tweet in page.data:\n",
        "    temp_df = pd.json_normalize(tweet.data, sep=\"_\")\n",
        "    netflix_tweets = netflix_tweets.append(temp_df, ignore_index=True)  # append to df_tweets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "jZBjFCWLqdZY",
        "outputId": "45fbb656-0d83-4d5f-e3dc-f78c288f4ab8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              tweet_id              user_id               date_posted  \\\n",
              "0  1633331695096336387  1570617299501199360  2023-03-08T04:59:56.000Z   \n",
              "1  1633331688901332993  1486722581990313985  2023-03-08T04:59:55.000Z   \n",
              "2  1633331680957329408  1012961249741529088  2023-03-08T04:59:53.000Z   \n",
              "3  1633331679938084864  1413901955928190979  2023-03-08T04:59:52.000Z   \n",
              "4  1633331670744199174  1570617299501199360  2023-03-08T04:59:50.000Z   \n",
              "\n",
              "   retweet_count  reply_count  like_count  quotetweet_count  impression_count  \\\n",
              "0              0            0           0                 0                 1   \n",
              "1              0          130           7                20               632   \n",
              "2              0            0           0                 0                 1   \n",
              "3              0            0           0                 0                 1   \n",
              "4              0            0           0                 0                 1   \n",
              "\n",
              "                                                text  \n",
              "0  @daisybigwins Sini dm kak aku jual fast\\nDi si...  \n",
              "1            Need Netflix 1 bulan\\n#zonaba #zonauang  \n",
              "2  üìå Available Premium Accounts \\n\\nüî∏Netflix\\nüî∏Sp...  \n",
              "3  @piccalaa halooo, @ballerinya jual apk premium...  \n",
              "4  @winterrrsun Sini dm kak aku jual fast\\nDi sin...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5d610f56-338a-4bef-8f3e-506d52ada707\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>user_id</th>\n",
              "      <th>date_posted</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>reply_count</th>\n",
              "      <th>like_count</th>\n",
              "      <th>quotetweet_count</th>\n",
              "      <th>impression_count</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1633331695096336387</td>\n",
              "      <td>1570617299501199360</td>\n",
              "      <td>2023-03-08T04:59:56.000Z</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>@daisybigwins Sini dm kak aku jual fast\\nDi si...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1633331688901332993</td>\n",
              "      <td>1486722581990313985</td>\n",
              "      <td>2023-03-08T04:59:55.000Z</td>\n",
              "      <td>0</td>\n",
              "      <td>130</td>\n",
              "      <td>7</td>\n",
              "      <td>20</td>\n",
              "      <td>632</td>\n",
              "      <td>Need Netflix 1 bulan\\n#zonaba #zonauang</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1633331680957329408</td>\n",
              "      <td>1012961249741529088</td>\n",
              "      <td>2023-03-08T04:59:53.000Z</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>üìå Available Premium Accounts \\n\\nüî∏Netflix\\nüî∏Sp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1633331679938084864</td>\n",
              "      <td>1413901955928190979</td>\n",
              "      <td>2023-03-08T04:59:52.000Z</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>@piccalaa halooo, @ballerinya jual apk premium...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1633331670744199174</td>\n",
              "      <td>1570617299501199360</td>\n",
              "      <td>2023-03-08T04:59:50.000Z</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>@winterrrsun Sini dm kak aku jual fast\\nDi sin...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5d610f56-338a-4bef-8f3e-506d52ada707')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5d610f56-338a-4bef-8f3e-506d52ada707 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5d610f56-338a-4bef-8f3e-506d52ada707');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "#drop, rearrange, and rename columns\n",
        "#netflix_tweets.drop(columns = 'edit_history_tweet_ids')\n",
        "netflix_tweets = netflix_tweets.rename(columns={'id': 'tweet_id', 'author_id': 'user_id', 'created_at': 'date_posted',\n",
        "                                    'public_metrics_retweet_count': 'retweet_count', 'public_metrics_reply_count': 'reply_count',\n",
        "                                    'public_metrics_like_count': 'like_count', 'public_metrics_quote_count': 'quotetweet_count',\n",
        "                                    'public_metrics_impression_count':'impression_count'})\n",
        "\n",
        "netflix_tweets = netflix_tweets[['tweet_id', 'user_id', 'date_posted','retweet_count', 'reply_count', 'like_count', 'quotetweet_count',\n",
        "                     'impression_count','text']]\n",
        "netflix_tweets.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20L_uNXvilVW",
        "outputId": "ea19a641-1132-472c-8ce3-b508e8739a7c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5923, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "netflix_tweets.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "wncFAssIp3Gz",
        "outputId": "8eff6451-c1f3-4c37-fd9f-d7ea0d843561"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a1509ef2-fd47-42da-a7f1-09bafbcb026a\", \"Netflix.csv\", 1581356)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#download the user dataframe as a .csv file\n",
        "netflix_tweets.to_csv('Netflix.csv', index=False)\n",
        "files.download('Netflix.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wk5qx_r7ifvD"
      },
      "source": [
        "###Hulu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0tbVOnCqG-c"
      },
      "outputs": [],
      "source": [
        "# Create an empty Pandas DataFrame\n",
        "hulu_tweets = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LE5NLADwim6u"
      },
      "outputs": [],
      "source": [
        "#search tweets by page limit\n",
        "page_search = tweepy.Paginator(\n",
        "    method = client.search_recent_tweets,\n",
        "    limit=10,\n",
        "    query=\"Hulu lang:en -is:retweet\",   #pulls all tweets that mention Dunkin and are in English, excludes retweets\n",
        "    expansions = [\"author_id\"],\n",
        "\n",
        "    ###change these dates to required time range###\n",
        "    start_time = \"2023-03-07T00:00:00-05:00\",\n",
        "    end_time = \"2023-03-07T23:59:59-05:00\",\n",
        "    ###############################################\n",
        "\n",
        "    tweet_fields = [\"created_at\", \"id\",\"public_metrics\", \"text\"], #information to be store from each tweet\n",
        "    max_results=100  #limits the number of tweets pulled\n",
        "    )\n",
        "\n",
        "# Iterate through each page of results\n",
        "for n,page in enumerate(page_search):\n",
        "\n",
        "  # Print the current page number\n",
        "  #print(\"PAGE NUMBER: \",n)\n",
        "\n",
        "  # Iterate through each tweet in the page.data, convert to a dataframe, and\n",
        "\n",
        "  for tweet in page.data:\n",
        "    temp_df = pd.json_normalize(tweet.data, sep=\"_\")\n",
        "    hulu_tweets = hulu_tweets.append(temp_df, ignore_index=True)  # append to df_tweets\n",
        "\n",
        "\n",
        "###############################################\n",
        "###############################################\n",
        "\n",
        "#search tweets by page limit\n",
        "page_search = tweepy.Paginator(\n",
        "    method = client.search_recent_tweets,\n",
        "    limit=10,\n",
        "    query=\"Hulu lang:en -is:retweet\",   #pulls all tweets that mention Dunkin and are in English, excludes retweets\n",
        "    expansions = [\"author_id\"],\n",
        "\n",
        "    ###change these dates to required time range###\n",
        "    start_time = \"2023-03-08T00:00:00-05:00\",\n",
        "    end_time = \"2023-03-08T23:59:59-05:00\",\n",
        "    ###############################################\n",
        "\n",
        "    tweet_fields = [\"created_at\", \"id\",\"public_metrics\", \"text\"], #information to be store from each tweet\n",
        "    max_results=100  #limits the number of tweets pulled\n",
        "    )\n",
        "\n",
        "# Iterate through each page of results\n",
        "for n,page in enumerate(page_search):\n",
        "\n",
        "  # Print the current page number\n",
        "  # print(\"PAGE NUMBER: \",n)\n",
        "\n",
        "  # Iterate through each tweet in the page.data, convert to a dataframe, and\n",
        "\n",
        "  for tweet in page.data:\n",
        "    temp_df = pd.json_normalize(tweet.data, sep=\"_\")\n",
        "    hulu_tweets = hulu_tweets.append(temp_df, ignore_index=True)  # append to df_tweets\n",
        "\n",
        "###############################################\n",
        "###############################################\n",
        "\n",
        "\n",
        "#search tweets by page limit\n",
        "page_search = tweepy.Paginator(\n",
        "    method = client.search_recent_tweets,\n",
        "    limit=10,\n",
        "    query=\"Hulu lang:en -is:retweet\",   #pulls all tweets that mention Dunkin and are in English, excludes retweets\n",
        "    expansions = [\"author_id\"],\n",
        "\n",
        "    ###change these dates to required time range###\n",
        "    start_time = \"2023-03-09T00:00:00-05:00\",\n",
        "    end_time = \"2023-03-09T23:59:59-05:00\",\n",
        "    ###############################################\n",
        "\n",
        "    tweet_fields = [\"created_at\", \"id\",\"public_metrics\", \"text\"], #information to be store from each tweet\n",
        "    max_results=100  #limits the number of tweets pulled\n",
        "    )\n",
        "\n",
        "# Iterate through each page of results\n",
        "for n,page in enumerate(page_search):\n",
        "\n",
        "  # Print the current page number\n",
        "  #print(\"PAGE NUMBER: \",n)\n",
        "\n",
        "  # Iterate through each tweet in the page.data, convert to a dataframe, and\n",
        "\n",
        "  for tweet in page.data:\n",
        "    temp_df = pd.json_normalize(tweet.data, sep=\"_\")\n",
        "    hulu_tweets = hulu_tweets.append(temp_df, ignore_index=True)  # append to df_tweets\n",
        "\n",
        "\n",
        "###############################################\n",
        "###############################################\n",
        "\n",
        "\n",
        "#search tweets by page limit\n",
        "page_search = tweepy.Paginator(\n",
        "    method = client.search_recent_tweets,\n",
        "    limit=10,\n",
        "    query=\"Hulu lang:en -is:retweet\",   #pulls all tweets that mention Dunkin and are in English, excludes retweets\n",
        "    expansions = [\"author_id\"],\n",
        "\n",
        "    ###change these dates to required time range###\n",
        "    start_time = \"2023-03-10T00:00:00-05:00\",\n",
        "    end_time = \"2023-03-10T23:59:59-05:00\",\n",
        "    ###############################################\n",
        "\n",
        "    tweet_fields = [\"created_at\", \"id\",\"public_metrics\", \"text\"], #information to be store from each tweet\n",
        "    max_results=100  #limits the number of tweets pulled\n",
        "    )\n",
        "\n",
        "# Iterate through each page of results\n",
        "for n,page in enumerate(page_search):\n",
        "\n",
        "  # Print the current page number\n",
        "  #print(\"PAGE NUMBER: \",n)\n",
        "\n",
        "  # Iterate through each tweet in the page.data, convert to a dataframe, and\n",
        "\n",
        "  for tweet in page.data:\n",
        "    temp_df = pd.json_normalize(tweet.data, sep=\"_\")\n",
        "    hulu_tweets = hulu_tweets.append(temp_df, ignore_index=True)  # append to df_tweets\n",
        "\n",
        "\n",
        "###############################################\n",
        "###############################################\n",
        "\n",
        "\n",
        "#search tweets by page limit\n",
        "page_search = tweepy.Paginator(\n",
        "    method = client.search_recent_tweets,\n",
        "    limit=10,\n",
        "    query=\"Hulu lang:en -is:retweet\",   #pulls all tweets that mention Dunkin and are in English, excludes retweets\n",
        "    expansions = [\"author_id\"],\n",
        "\n",
        "    ###change these dates to required time range###\n",
        "    start_time = \"2023-03-11T00:00:00-05:00\",\n",
        "    end_time = \"2023-03-11T23:59:59-05:00\",\n",
        "    ###############################################\n",
        "\n",
        "    tweet_fields = [\"created_at\", \"id\",\"public_metrics\", \"text\"], #information to be store from each tweet\n",
        "    max_results=100  #limits the number of tweets pulled\n",
        "    )\n",
        "\n",
        "# Iterate through each page of results\n",
        "for n,page in enumerate(page_search):\n",
        "\n",
        "  # Print the current page number\n",
        "  #print(\"PAGE NUMBER: \",n)\n",
        "\n",
        "  # Iterate through each tweet in the page.data, convert to a dataframe, and\n",
        "\n",
        "  for tweet in page.data:\n",
        "    temp_df = pd.json_normalize(tweet.data, sep=\"_\")\n",
        "    hulu_tweets = hulu_tweets.append(temp_df, ignore_index=True)  # append to df_tweets\n",
        "\n",
        "\n",
        "\n",
        "###############################################\n",
        "###############################################\n",
        "\n",
        "\n",
        "#search tweets by page limit\n",
        "page_search = tweepy.Paginator(\n",
        "    method = client.search_recent_tweets,\n",
        "    limit=10,\n",
        "    query=\"Hulu lang:en -is:retweet\",   #pulls all tweets that mention Dunkin and are in English, excludes retweets\n",
        "    expansions = [\"author_id\"],\n",
        "\n",
        "    ###change these dates to required time range###\n",
        "    start_time = \"2023-03-12T00:00:00-05:00\",\n",
        "    end_time = \"2023-03-12T23:59:59-05:00\",\n",
        "    ###############################################\n",
        "\n",
        "    tweet_fields = [\"created_at\", \"id\",\"public_metrics\", \"text\"], #information to be store from each tweet\n",
        "    max_results=100  #limits the number of tweets pulled\n",
        "    )\n",
        "\n",
        "# Iterate through each page of results\n",
        "for n,page in enumerate(page_search):\n",
        "\n",
        "  # Print the current page number\n",
        "  #print(\"PAGE NUMBER: \",n)\n",
        "\n",
        "  # Iterate through each tweet in the page.data, convert to a dataframe, and\n",
        "\n",
        "  for tweet in page.data:\n",
        "    temp_df = pd.json_normalize(tweet.data, sep=\"_\")\n",
        "    hulu_tweets = hulu_tweets.append(temp_df, ignore_index=True)  # append to df_tweets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "SRoTTXjHqK3r",
        "outputId": "2baca61c-3df1-4203-b5b6-855419576a77"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              tweet_id              user_id               date_posted  \\\n",
              "0  1633331288903159809  1559930535526354944  2023-03-08T04:58:19.000Z   \n",
              "1  1633331031016128515             16567309  2023-03-08T04:57:18.000Z   \n",
              "2  1633330887822548995  1314339641164812288  2023-03-08T04:56:44.000Z   \n",
              "3  1633330855295832064            935901733  2023-03-08T04:56:36.000Z   \n",
              "4  1633330682335498240   841510254504370176  2023-03-08T04:55:55.000Z   \n",
              "\n",
              "   retweet_count  reply_count  like_count  quotetweet_count  impression_count  \\\n",
              "0              0            0           0                 0                 5   \n",
              "1              0            0           0                 1                56   \n",
              "2              0            1           1                 0               106   \n",
              "3              0            1           1                 0               116   \n",
              "4              0            1           0                 0                17   \n",
              "\n",
              "                                                text  \n",
              "0          @DramaClubFOX @FOXTV @hulu Writers unite?  \n",
              "1  @busybaud Sorry for any upset! We suggest usin...  \n",
              "2  Hey, @hulu. Is there a way tone it down with a...  \n",
              "3  @wlstleblower Yes, I have \"Hulu with Live TV.\"...  \n",
              "4  @d0yoxngii 5 dollars a month for a bundle that...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-916a613a-bdde-4775-ad20-cf7e97209a6f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>user_id</th>\n",
              "      <th>date_posted</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>reply_count</th>\n",
              "      <th>like_count</th>\n",
              "      <th>quotetweet_count</th>\n",
              "      <th>impression_count</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1633331288903159809</td>\n",
              "      <td>1559930535526354944</td>\n",
              "      <td>2023-03-08T04:58:19.000Z</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>@DramaClubFOX @FOXTV @hulu Writers unite?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1633331031016128515</td>\n",
              "      <td>16567309</td>\n",
              "      <td>2023-03-08T04:57:18.000Z</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>56</td>\n",
              "      <td>@busybaud Sorry for any upset! We suggest usin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1633330887822548995</td>\n",
              "      <td>1314339641164812288</td>\n",
              "      <td>2023-03-08T04:56:44.000Z</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>106</td>\n",
              "      <td>Hey, @hulu. Is there a way tone it down with a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1633330855295832064</td>\n",
              "      <td>935901733</td>\n",
              "      <td>2023-03-08T04:56:36.000Z</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>116</td>\n",
              "      <td>@wlstleblower Yes, I have \"Hulu with Live TV.\"...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1633330682335498240</td>\n",
              "      <td>841510254504370176</td>\n",
              "      <td>2023-03-08T04:55:55.000Z</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>@d0yoxngii 5 dollars a month for a bundle that...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-916a613a-bdde-4775-ad20-cf7e97209a6f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-916a613a-bdde-4775-ad20-cf7e97209a6f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-916a613a-bdde-4775-ad20-cf7e97209a6f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "#drop, rearrange, and rename columns\n",
        "hulu_tweets.drop(columns = 'edit_history_tweet_ids')\n",
        "hulu_tweets = hulu_tweets.rename(columns={'id': 'tweet_id', 'author_id': 'user_id', 'created_at': 'date_posted',\n",
        "                                    'public_metrics_retweet_count': 'retweet_count', 'public_metrics_reply_count': 'reply_count',\n",
        "                                    'public_metrics_like_count': 'like_count', 'public_metrics_quote_count': 'quotetweet_count',\n",
        "                                    'public_metrics_impression_count':'impression_count'})\n",
        "\n",
        "hulu_tweets = hulu_tweets[['tweet_id', 'user_id', 'date_posted','retweet_count', 'reply_count', 'like_count', 'quotetweet_count',\n",
        "                     'impression_count','text']]\n",
        "hulu_tweets.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niEgfmAkim9T",
        "outputId": "a262612c-2c2d-4a25-cb68-05883805e8a2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5960, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "hulu_tweets.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "NdROnYoJinAH",
        "outputId": "0dbf449a-3c78-41f7-efd5-56ccf3aa0bc8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c9ac2ac9-9925-4711-9c5a-759dded4989d\", \"Hulu.csv\", 1220899)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#download the user dataframe as a .csv file\n",
        "hulu_tweets.to_csv('Hulu.csv', index=False)\n",
        "files.download('Hulu.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7bpajnJb-_y"
      },
      "source": [
        "###Southwest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c65yt31TrTUT"
      },
      "outputs": [],
      "source": [
        "# Create an empty Pandas DataFrame\n",
        "southwest_tweets = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRkJtbM0cA5t"
      },
      "outputs": [],
      "source": [
        "#search tweets by page limit\n",
        "page_search = tweepy.Paginator(\n",
        "    method = client.search_recent_tweets,\n",
        "    limit=10,\n",
        "    query=\"Southwest Airline lang:en -is:retweet\",   #pulls all tweets that mention Dunkin and are in English, excludes retweets\n",
        "    expansions = [\"author_id\"],\n",
        "\n",
        "    ###change these dates to required time range###\n",
        "    start_time = \"2023-03-07T00:00:00-05:00\",\n",
        "    end_time = \"2023-03-07T23:59:59-05:00\",\n",
        "    ###############################################\n",
        "\n",
        "    tweet_fields = [\"created_at\", \"id\",\"public_metrics\", \"text\"], #information to be store from each tweet\n",
        "    max_results=100  #limits the number of tweets pulled\n",
        "    )\n",
        "\n",
        "# Iterate through each page of results\n",
        "for n,page in enumerate(page_search):\n",
        "\n",
        "  # Print the current page number\n",
        "  #print(\"PAGE NUMBER: \",n)\n",
        "\n",
        "  # Iterate through each tweet in the page.data, convert to a dataframe, and\n",
        "\n",
        "  for tweet in page.data:\n",
        "    temp_df = pd.json_normalize(tweet.data, sep=\"_\")\n",
        "    southwest_tweets = southwest_tweets.append(temp_df, ignore_index=True)  # append to df_tweets\n",
        "\n",
        "\n",
        "###############################################\n",
        "###############################################\n",
        "\n",
        "#search tweets by page limit\n",
        "page_search = tweepy.Paginator(\n",
        "    method = client.search_recent_tweets,\n",
        "    limit=10,\n",
        "    query=\"Southwest Airline lang:en -is:retweet\",   #pulls all tweets that mention Dunkin and are in English, excludes retweets\n",
        "    expansions = [\"author_id\"],\n",
        "\n",
        "    ###change these dates to required time range###\n",
        "    start_time = \"2023-03-08T00:00:00-05:00\",\n",
        "    end_time = \"2023-03-08T23:59:59-05:00\",\n",
        "    ###############################################\n",
        "\n",
        "    tweet_fields = [\"created_at\", \"id\",\"public_metrics\", \"text\"], #information to be store from each tweet\n",
        "    max_results=100  #limits the number of tweets pulled\n",
        "    )\n",
        "\n",
        "# Iterate through each page of results\n",
        "for n,page in enumerate(page_search):\n",
        "\n",
        "  # Print the current page number\n",
        "  # print(\"PAGE NUMBER: \",n)\n",
        "\n",
        "  # Iterate through each tweet in the page.data, convert to a dataframe, and\n",
        "\n",
        "  for tweet in page.data:\n",
        "    temp_df = pd.json_normalize(tweet.data, sep=\"_\")\n",
        "    southwest_tweets = southwest_tweets.append(temp_df, ignore_index=True)  # append to df_tweets\n",
        "\n",
        "###############################################\n",
        "###############################################\n",
        "\n",
        "\n",
        "#search tweets by page limit\n",
        "page_search = tweepy.Paginator(\n",
        "    method = client.search_recent_tweets,\n",
        "    limit=10,\n",
        "    query=\"Southwest Airline lang:en -is:retweet\",   #pulls all tweets that mention Dunkin and are in English, excludes retweets\n",
        "    expansions = [\"author_id\"],\n",
        "\n",
        "    ###change these dates to required time range###\n",
        "    start_time = \"2023-03-09T00:00:00-05:00\",\n",
        "    end_time = \"2023-03-09T23:59:59-05:00\",\n",
        "    ###############################################\n",
        "\n",
        "    tweet_fields = [\"created_at\", \"id\",\"public_metrics\", \"text\"], #information to be store from each tweet\n",
        "    max_results=100  #limits the number of tweets pulled\n",
        "    )\n",
        "\n",
        "# Iterate through each page of results\n",
        "for n,page in enumerate(page_search):\n",
        "\n",
        "  # Print the current page number\n",
        "  #print(\"PAGE NUMBER: \",n)\n",
        "\n",
        "  # Iterate through each tweet in the page.data, convert to a dataframe, and\n",
        "\n",
        "  for tweet in page.data:\n",
        "    temp_df = pd.json_normalize(tweet.data, sep=\"_\")\n",
        "    southwest_tweets = southwest_tweets.append(temp_df, ignore_index=True)  # append to df_tweets\n",
        "\n",
        "\n",
        "###############################################\n",
        "###############################################\n",
        "\n",
        "\n",
        "#search tweets by page limit\n",
        "page_search = tweepy.Paginator(\n",
        "    method = client.search_recent_tweets,\n",
        "    limit=10,\n",
        "    query=\"Southwest Airline lang:en -is:retweet\",   #pulls all tweets that mention Dunkin and are in English, excludes retweets\n",
        "    expansions = [\"author_id\"],\n",
        "\n",
        "    ###change these dates to required time range###\n",
        "    start_time = \"2023-03-10T00:00:00-05:00\",\n",
        "    end_time = \"2023-03-10T23:59:59-05:00\",\n",
        "    ###############################################\n",
        "\n",
        "    tweet_fields = [\"created_at\", \"id\",\"public_metrics\", \"text\"], #information to be store from each tweet\n",
        "    max_results=100  #limits the number of tweets pulled\n",
        "    )\n",
        "\n",
        "# Iterate through each page of results\n",
        "for n,page in enumerate(page_search):\n",
        "\n",
        "  # Print the current page number\n",
        "  #print(\"PAGE NUMBER: \",n)\n",
        "\n",
        "  # Iterate through each tweet in the page.data, convert to a dataframe, and\n",
        "\n",
        "  for tweet in page.data:\n",
        "    temp_df = pd.json_normalize(tweet.data, sep=\"_\")\n",
        "    southwest_tweets = southwest_tweets.append(temp_df, ignore_index=True)  # append to df_tweets\n",
        "\n",
        "\n",
        "###############################################\n",
        "###############################################\n",
        "\n",
        "\n",
        "#search tweets by page limit\n",
        "page_search = tweepy.Paginator(\n",
        "    method = client.search_recent_tweets,\n",
        "    limit=10,\n",
        "    query=\"Southwest Airline lang:en -is:retweet\",   #pulls all tweets that mention Dunkin and are in English, excludes retweets\n",
        "    expansions = [\"author_id\"],\n",
        "\n",
        "    ###change these dates to required time range###\n",
        "    start_time = \"2023-03-11T00:00:00-05:00\",\n",
        "    end_time = \"2023-03-11T23:59:59-05:00\",\n",
        "    ###############################################\n",
        "\n",
        "    tweet_fields = [\"created_at\", \"id\",\"public_metrics\", \"text\"], #information to be store from each tweet\n",
        "    max_results=100  #limits the number of tweets pulled\n",
        "    )\n",
        "\n",
        "# Iterate through each page of results\n",
        "for n,page in enumerate(page_search):\n",
        "\n",
        "  # Print the current page number\n",
        "  #print(\"PAGE NUMBER: \",n)\n",
        "\n",
        "  # Iterate through each tweet in the page.data, convert to a dataframe, and\n",
        "\n",
        "  for tweet in page.data:\n",
        "    temp_df = pd.json_normalize(tweet.data, sep=\"_\")\n",
        "    southwest_tweets = southwest_tweets.append(temp_df, ignore_index=True)  # append to df_tweets\n",
        "\n",
        "\n",
        "###############################################\n",
        "###############################################\n",
        "\n",
        "\n",
        "#search tweets by page limit\n",
        "page_search = tweepy.Paginator(\n",
        "    method = client.search_recent_tweets,\n",
        "    limit=10,\n",
        "    query=\"Southwest Airline lang:en -is:retweet\",   #pulls all tweets that mention Dunkin and are in English, excludes retweets\n",
        "    expansions = [\"author_id\"],\n",
        "\n",
        "    ###change these dates to required time range###\n",
        "    start_time = \"2023-03-12T00:00:00-05:00\",\n",
        "    end_time = \"2023-03-12T23:59:59-05:00\",\n",
        "    ###############################################\n",
        "\n",
        "    tweet_fields = [\"created_at\", \"id\",\"public_metrics\", \"text\"], #information to be store from each tweet\n",
        "    max_results=100  #limits the number of tweets pulled\n",
        "    )\n",
        "\n",
        "# Iterate through each page of results\n",
        "for n,page in enumerate(page_search):\n",
        "\n",
        "  # Print the current page number\n",
        "  #print(\"PAGE NUMBER: \",n)\n",
        "\n",
        "  # Iterate through each tweet in the page.data, convert to a dataframe, and\n",
        "\n",
        "  for tweet in page.data:\n",
        "    temp_df = pd.json_normalize(tweet.data, sep=\"_\")\n",
        "    southwest_tweets = southwest_tweets.append(temp_df, ignore_index=True)  # append to df_tweets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "rWDuewqSrUx1",
        "outputId": "87e0a5a8-efc3-4f6a-fca1-82d1435a07f8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              tweet_id              user_id               date_posted  \\\n",
              "0  1633298278623203328  1052953940361383937  2023-03-08T02:47:09.000Z   \n",
              "1  1633288381538140160  1333695925257060352  2023-03-08T02:07:49.000Z   \n",
              "2  1633288259890475010            623618282  2023-03-08T02:07:20.000Z   \n",
              "3  1633284622845898754             21681690  2023-03-08T01:52:53.000Z   \n",
              "4  1633276893465026561           2429612832  2023-03-08T01:22:10.000Z   \n",
              "\n",
              "   retweet_count  reply_count  like_count  quotetweet_count  impression_count  \\\n",
              "0              2            3          19                 0              3246   \n",
              "1              0            0           0                 0                68   \n",
              "2              0            0           7                 0               276   \n",
              "3              0            0           3                 0               149   \n",
              "4              0            0           0                 0                60   \n",
              "\n",
              "                                                text  \n",
              "0  Southwest airline from Cuba to Florida https:/...  \n",
              "1  Fight breaks out on Southwest flight before pa...  \n",
              "2  I can‚Äôt even imagine who the fuck cares about ...  \n",
              "3  @SecretaryPete Most airline mergers are anti-c...  \n",
              "4  Fight breaks out on Southwest flight before pa...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9ebcb03b-940d-4b0d-9ca7-39da124f60cd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>user_id</th>\n",
              "      <th>date_posted</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>reply_count</th>\n",
              "      <th>like_count</th>\n",
              "      <th>quotetweet_count</th>\n",
              "      <th>impression_count</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1633298278623203328</td>\n",
              "      <td>1052953940361383937</td>\n",
              "      <td>2023-03-08T02:47:09.000Z</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>3246</td>\n",
              "      <td>Southwest airline from Cuba to Florida https:/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1633288381538140160</td>\n",
              "      <td>1333695925257060352</td>\n",
              "      <td>2023-03-08T02:07:49.000Z</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>68</td>\n",
              "      <td>Fight breaks out on Southwest flight before pa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1633288259890475010</td>\n",
              "      <td>623618282</td>\n",
              "      <td>2023-03-08T02:07:20.000Z</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>276</td>\n",
              "      <td>I can‚Äôt even imagine who the fuck cares about ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1633284622845898754</td>\n",
              "      <td>21681690</td>\n",
              "      <td>2023-03-08T01:52:53.000Z</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>149</td>\n",
              "      <td>@SecretaryPete Most airline mergers are anti-c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1633276893465026561</td>\n",
              "      <td>2429612832</td>\n",
              "      <td>2023-03-08T01:22:10.000Z</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>60</td>\n",
              "      <td>Fight breaks out on Southwest flight before pa...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9ebcb03b-940d-4b0d-9ca7-39da124f60cd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9ebcb03b-940d-4b0d-9ca7-39da124f60cd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9ebcb03b-940d-4b0d-9ca7-39da124f60cd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "#drop, rearrange, and rename columns\n",
        "southwest_tweets.drop(columns = 'edit_history_tweet_ids')\n",
        "southwest_tweets = southwest_tweets.rename(columns={'id': 'tweet_id', 'author_id': 'user_id', 'created_at': 'date_posted',\n",
        "                                    'public_metrics_retweet_count': 'retweet_count', 'public_metrics_reply_count': 'reply_count',\n",
        "                                    'public_metrics_like_count': 'like_count', 'public_metrics_quote_count': 'quotetweet_count',\n",
        "                                    'public_metrics_impression_count':'impression_count'})\n",
        "\n",
        "southwest_tweets = southwest_tweets[['tweet_id', 'user_id', 'date_posted','retweet_count', 'reply_count', 'like_count', 'quotetweet_count',\n",
        "                     'impression_count','text']]\n",
        "southwest_tweets.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9j3fX0lcA8Q",
        "outputId": "e66ac2f2-1d8a-4711-bf2c-26911535ae51"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(287, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "southwest_tweets.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "kw2dvoIycA_J",
        "outputId": "aaf9cbd6-39ec-41c4-b0f5-07c6714ecb31"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_1d52ce2c-b923-47e2-9f6b-82e5102c78d2\", \"Southwest.csv\", 73255)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#download the user dataframe as a .csv file\n",
        "southwest_tweets.to_csv('Southwest.csv', index=False)\n",
        "files.download('Southwest.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0ADQmWccBJ2"
      },
      "source": [
        "###Delta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hdl5yObJrYO7"
      },
      "outputs": [],
      "source": [
        "# Create an empty Pandas DataFrame\n",
        "delta_tweets = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUWMgCjqcC9g"
      },
      "outputs": [],
      "source": [
        "#search tweets by page limit\n",
        "page_search = tweepy.Paginator(\n",
        "    method = client.search_recent_tweets,\n",
        "    limit=10,\n",
        "    query=\"Delta Airline lang:en -is:retweet\",   #pulls all tweets that mention Dunkin and are in English, excludes retweets\n",
        "    expansions = [\"author_id\"],\n",
        "\n",
        "    ###change these dates to required time range###\n",
        "    start_time = \"2023-03-07T00:00:00-05:00\",\n",
        "    end_time = \"2023-03-07T23:59:59-05:00\",\n",
        "    ###############################################\n",
        "\n",
        "    tweet_fields = [\"created_at\", \"id\",\"public_metrics\", \"text\"], #information to be store from each tweet\n",
        "    max_results=100  #limits the number of tweets pulled\n",
        "    )\n",
        "\n",
        "# Iterate through each page of results\n",
        "for n,page in enumerate(page_search):\n",
        "\n",
        "  # Print the current page number\n",
        "  #print(\"PAGE NUMBER: \",n)\n",
        "\n",
        "  # Iterate through each tweet in the page.data, convert to a dataframe, and\n",
        "\n",
        "  for tweet in page.data:\n",
        "    temp_df = pd.json_normalize(tweet.data, sep=\"_\")\n",
        "    delta_tweets = delta_tweets.append(temp_df, ignore_index=True)  # append to df_tweets\n",
        "\n",
        "\n",
        "###############################################\n",
        "###############################################\n",
        "\n",
        "#search tweets by page limit\n",
        "page_search = tweepy.Paginator(\n",
        "    method = client.search_recent_tweets,\n",
        "    limit=10,\n",
        "    query=\"Delta Airline lang:en -is:retweet\",   #pulls all tweets that mention Dunkin and are in English, excludes retweets\n",
        "    expansions = [\"author_id\"],\n",
        "\n",
        "    ###change these dates to required time range###\n",
        "    start_time = \"2023-03-08T00:00:00-05:00\",\n",
        "    end_time = \"2023-03-08T23:59:59-05:00\",\n",
        "    ###############################################\n",
        "\n",
        "    tweet_fields = [\"created_at\", \"id\",\"public_metrics\", \"text\"], #information to be store from each tweet\n",
        "    max_results=100  #limits the number of tweets pulled\n",
        "    )\n",
        "\n",
        "# Iterate through each page of results\n",
        "for n,page in enumerate(page_search):\n",
        "\n",
        "  # Print the current page number\n",
        "  # print(\"PAGE NUMBER: \",n)\n",
        "\n",
        "  # Iterate through each tweet in the page.data, convert to a dataframe, and\n",
        "\n",
        "  for tweet in page.data:\n",
        "    temp_df = pd.json_normalize(tweet.data, sep=\"_\")\n",
        "    delta_tweets = delta_tweets.append(temp_df, ignore_index=True)  # append to df_tweets\n",
        "\n",
        "###############################################\n",
        "###############################################\n",
        "\n",
        "\n",
        "#search tweets by page limit\n",
        "page_search = tweepy.Paginator(\n",
        "    method = client.search_recent_tweets,\n",
        "    limit=10,\n",
        "    query=\"Delta Airline lang:en -is:retweet\",   #pulls all tweets that mention Dunkin and are in English, excludes retweets\n",
        "    expansions = [\"author_id\"],\n",
        "\n",
        "    ###change these dates to required time range###\n",
        "    start_time = \"2023-03-09T00:00:00-05:00\",\n",
        "    end_time = \"2023-03-09T23:59:59-05:00\",\n",
        "    ###############################################\n",
        "\n",
        "    tweet_fields = [\"created_at\", \"id\",\"public_metrics\", \"text\"], #information to be store from each tweet\n",
        "    max_results=100  #limits the number of tweets pulled\n",
        "    )\n",
        "\n",
        "# Iterate through each page of results\n",
        "for n,page in enumerate(page_search):\n",
        "\n",
        "  # Print the current page number\n",
        "  #print(\"PAGE NUMBER: \",n)\n",
        "\n",
        "  # Iterate through each tweet in the page.data, convert to a dataframe, and\n",
        "\n",
        "  for tweet in page.data:\n",
        "    temp_df = pd.json_normalize(tweet.data, sep=\"_\")\n",
        "    delta_tweets = delta_tweets.append(temp_df, ignore_index=True)  # append to df_tweets\n",
        "\n",
        "\n",
        "###############################################\n",
        "###############################################\n",
        "\n",
        "\n",
        "#search tweets by page limit\n",
        "page_search = tweepy.Paginator(\n",
        "    method = client.search_recent_tweets,\n",
        "    limit=10,\n",
        "    query=\"Delta Airline lang:en -is:retweet\",   #pulls all tweets that mention Dunkin and are in English, excludes retweets\n",
        "    expansions = [\"author_id\"],\n",
        "\n",
        "    ###change these dates to required time range###\n",
        "    start_time = \"2023-03-10T00:00:00-05:00\",\n",
        "    end_time = \"2023-03-10T23:59:59-05:00\",\n",
        "    ###############################################\n",
        "\n",
        "    tweet_fields = [\"created_at\", \"id\",\"public_metrics\", \"text\"], #information to be store from each tweet\n",
        "    max_results=100  #limits the number of tweets pulled\n",
        "    )\n",
        "\n",
        "# Iterate through each page of results\n",
        "for n,page in enumerate(page_search):\n",
        "\n",
        "  # Print the current page number\n",
        "  #print(\"PAGE NUMBER: \",n)\n",
        "\n",
        "  # Iterate through each tweet in the page.data, convert to a dataframe, and\n",
        "\n",
        "  for tweet in page.data:\n",
        "    temp_df = pd.json_normalize(tweet.data, sep=\"_\")\n",
        "    delta_tweets = delta_tweets.append(temp_df, ignore_index=True)  # append to df_tweets\n",
        "\n",
        "\n",
        "###############################################\n",
        "###############################################\n",
        "\n",
        "\n",
        "#search tweets by page limit\n",
        "page_search = tweepy.Paginator(\n",
        "    method = client.search_recent_tweets,\n",
        "    limit=10,\n",
        "    query=\"Delta Airline lang:en -is:retweet\",   #pulls all tweets that mention Dunkin and are in English, excludes retweets\n",
        "    expansions = [\"author_id\"],\n",
        "\n",
        "    ###change these dates to required time range###\n",
        "    start_time = \"2023-03-11T00:00:00-05:00\",\n",
        "    end_time = \"2023-03-11T23:59:59-05:00\",\n",
        "    ###############################################\n",
        "\n",
        "    tweet_fields = [\"created_at\", \"id\",\"public_metrics\", \"text\"], #information to be store from each tweet\n",
        "    max_results=100  #limits the number of tweets pulled\n",
        "    )\n",
        "\n",
        "# Iterate through each page of results\n",
        "for n,page in enumerate(page_search):\n",
        "\n",
        "  # Print the current page number\n",
        "  #print(\"PAGE NUMBER: \",n)\n",
        "\n",
        "  # Iterate through each tweet in the page.data, convert to a dataframe, and\n",
        "\n",
        "  for tweet in page.data:\n",
        "    temp_df = pd.json_normalize(tweet.data, sep=\"_\")\n",
        "    delta_tweets = delta_tweets.append(temp_df, ignore_index=True)  # append to df_tweets\n",
        "\n",
        "\n",
        "###############################################\n",
        "###############################################\n",
        "\n",
        "\n",
        "#search tweets by page limit\n",
        "page_search = tweepy.Paginator(\n",
        "    method = client.search_recent_tweets,\n",
        "    limit=10,\n",
        "    query=\"Delta Airline lang:en -is:retweet\",   #pulls all tweets that mention Dunkin and are in English, excludes retweets\n",
        "    expansions = [\"author_id\"],\n",
        "\n",
        "    ###change these dates to required time range###\n",
        "    start_time = \"2023-03-12T00:00:00-05:00\",\n",
        "    end_time = \"2023-03-12T23:59:59-05:00\",\n",
        "    ###############################################\n",
        "\n",
        "    tweet_fields = [\"created_at\", \"id\",\"public_metrics\", \"text\"], #information to be store from each tweet\n",
        "    max_results=100  #limits the number of tweets pulled\n",
        "    )\n",
        "\n",
        "# Iterate through each page of results\n",
        "for n,page in enumerate(page_search):\n",
        "\n",
        "  # Print the current page number\n",
        "  #print(\"PAGE NUMBER: \",n)\n",
        "\n",
        "  # Iterate through each tweet in the page.data, convert to a dataframe, and\n",
        "\n",
        "  for tweet in page.data:\n",
        "    temp_df = pd.json_normalize(tweet.data, sep=\"_\")\n",
        "    delta_tweets = delta_tweets.append(temp_df, ignore_index=True)  # append to df_tweets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVIHdRf9rZTO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "da77adf6-a517-4c1e-dc01-08f2bccf09ee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              tweet_id              user_id               date_posted  \\\n",
              "0  1633326641286897665            183109386  2023-03-08T04:39:51.000Z   \n",
              "1  1633324081045962753  1537969682807783425  2023-03-08T04:29:41.000Z   \n",
              "2  1633317632114937856  1620470009582551041  2023-03-08T04:04:03.000Z   \n",
              "3  1633315971401228290   968713339751817217  2023-03-08T03:57:27.000Z   \n",
              "4  1633292326419193856             59690099  2023-03-08T02:23:30.000Z   \n",
              "\n",
              "   retweet_count  reply_count  like_count  quotetweet_count  impression_count  \\\n",
              "0              0            0           0                 0                74   \n",
              "1              0            1           0                 0                44   \n",
              "2              0            2           0                 0                 8   \n",
              "3              0            1           0                 1                28   \n",
              "4              0            0           2                 0               125   \n",
              "\n",
              "                                                text  \n",
              "0  THIS SINGLE FLIGHT WITH DELTA HAS RESTORED MY ...  \n",
              "1  @victoriaaatx I have Amazon rn ü´†ü´† but I want i...  \n",
              "2  Gabe Horn Esq.\\n@gabehornesquire\\n¬∑\\n5m\\nReply...  \n",
              "3  @ACEXLMOM @NickAdamsinUSA I will not take this...  \n",
              "4  Delta the best airline but I‚Äôm flying whatever...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4e23f0de-232a-4f10-8e85-04abbfe22664\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>user_id</th>\n",
              "      <th>date_posted</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>reply_count</th>\n",
              "      <th>like_count</th>\n",
              "      <th>quotetweet_count</th>\n",
              "      <th>impression_count</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1633326641286897665</td>\n",
              "      <td>183109386</td>\n",
              "      <td>2023-03-08T04:39:51.000Z</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>74</td>\n",
              "      <td>THIS SINGLE FLIGHT WITH DELTA HAS RESTORED MY ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1633324081045962753</td>\n",
              "      <td>1537969682807783425</td>\n",
              "      <td>2023-03-08T04:29:41.000Z</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>44</td>\n",
              "      <td>@victoriaaatx I have Amazon rn ü´†ü´† but I want i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1633317632114937856</td>\n",
              "      <td>1620470009582551041</td>\n",
              "      <td>2023-03-08T04:04:03.000Z</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>Gabe Horn Esq.\\n@gabehornesquire\\n¬∑\\n5m\\nReply...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1633315971401228290</td>\n",
              "      <td>968713339751817217</td>\n",
              "      <td>2023-03-08T03:57:27.000Z</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>28</td>\n",
              "      <td>@ACEXLMOM @NickAdamsinUSA I will not take this...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1633292326419193856</td>\n",
              "      <td>59690099</td>\n",
              "      <td>2023-03-08T02:23:30.000Z</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>125</td>\n",
              "      <td>Delta the best airline but I‚Äôm flying whatever...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4e23f0de-232a-4f10-8e85-04abbfe22664')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4e23f0de-232a-4f10-8e85-04abbfe22664 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4e23f0de-232a-4f10-8e85-04abbfe22664');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "#drop, rearrange, and rename columns\n",
        "delta_tweets.drop(columns = 'edit_history_tweet_ids')\n",
        "delta_tweets = delta_tweets.rename(columns={'id': 'tweet_id', 'author_id': 'user_id', 'created_at': 'date_posted',\n",
        "                                    'public_metrics_retweet_count': 'retweet_count', 'public_metrics_reply_count': 'reply_count',\n",
        "                                    'public_metrics_like_count': 'like_count', 'public_metrics_quote_count': 'quotetweet_count',\n",
        "                                    'public_metrics_impression_count':'impression_count'})\n",
        "\n",
        "delta_tweets = delta_tweets[['tweet_id', 'user_id', 'date_posted','retweet_count', 'reply_count', 'like_count', 'quotetweet_count',\n",
        "                     'impression_count','text']]\n",
        "delta_tweets.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChUHMMJEcDAD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3551488c-3861-4be0-a76a-47780d70a2ca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(413, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "delta_tweets.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_URlQ_acDCq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "2611e254-c360-432b-857d-43edf06ccd05"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_32bd5cf1-05c8-4fe2-bd10-b403bc870a1b\", \"Delta.csv\", 106681)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#download the user dataframe as a .csv file\n",
        "delta_tweets.to_csv('Delta.csv', index=False)\n",
        "files.download('Delta.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqjBmJ12o-IZ"
      },
      "source": [
        "##User\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnuMHJGc3Tp-"
      },
      "outputs": [],
      "source": [
        "user = pd.read_csv('/content/Users_Week5.csv')\n",
        "user = user['user_id']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMFRR--x3Ekl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69ec6796-6232-4df7-f31b-0fca49e75c6e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13621,)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "user.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PWnPGBY2tdY"
      },
      "outputs": [],
      "source": [
        "user_list = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2igNqx8POLx",
        "outputId": "1a729653-115d-4268-ae1f-6544a1d99eaa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "range(0, 13621)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "range(len(user))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LVuQS-lpLCZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "f30600b1-4bc8-4015-8d95-e7008ccf7203"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tweepy.client:Rate limit exceeded. Sleeping for 748 seconds.\n",
            "WARNING:tweepy.client:Rate limit exceeded. Sleeping for 886 seconds.\n",
            "WARNING:tweepy.client:Rate limit exceeded. Sleeping for 888 seconds.\n",
            "WARNING:tweepy.client:Rate limit exceeded. Sleeping for 889 seconds.\n",
            "WARNING:tweepy.client:Rate limit exceeded. Sleeping for 885 seconds.\n",
            "WARNING:tweepy.client:Rate limit exceeded. Sleeping for 887 seconds.\n",
            "WARNING:tweepy.client:Rate limit exceeded. Sleeping for 886 seconds.\n",
            "WARNING:tweepy.client:Rate limit exceeded. Sleeping for 888 seconds.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_99532ba9-459a-4ddb-8c2e-a67d588d664d\", \"Users.csv\", 417075)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# collect data of the users\n",
        "for i in range(11370, len(user)):\n",
        "  users = client.get_user(id = user.iloc[i],\n",
        "                          user_fields = [\"created_at\", \"description\", \"id\", \"location\", \"name\", \"public_metrics\", \"username\", \"verified\", \"verified_type\"])\n",
        "  if users.data != None:\n",
        "    user_list.append(users.data.data)\n",
        "\n",
        "df_user = pd.json_normalize(user_list, sep=\"_\" )\n",
        "\n",
        "df_user = df_user.rename(columns={'created_at': 'date_joined', 'description': 'bio', 'id': 'user_id',\n",
        "                                    'public_metrics_followers_count': 'follower_count', 'public_metrics_following_count': 'following_count',\n",
        "                                    'public_metrics_listed_count': 'listed_count', 'public_metrics_tweet_count': 'tweet_count'})\n",
        "\n",
        "df_user = df_user[['name', 'user_id', 'username', 'date_joined', 'bio', 'location', 'verified',\n",
        "                     'verified_type','follower_count','following_count','listed_count','tweet_count']]\n",
        "df_user.to_csv('Users.csv', index=False)\n",
        "files.download('Users.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDKCeOOfpLFP"
      },
      "outputs": [],
      "source": [
        "df_user = pd.json_normalize(user_list, sep=\"_\" )\n",
        "df_user.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyIFPOyU3PmS",
        "outputId": "a0fdc087-4dde-4871-fb10-515388875a0b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11379, 12)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "df_user.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Erxp2JRb8uFb"
      },
      "outputs": [],
      "source": [
        "#rearrange and rename columns\n",
        "df_user = df_user.rename(columns={'created_at': 'date_joined', 'description': 'bio', 'id': 'user_id',\n",
        "                                    'public_metrics_followers_count': 'follower_count', 'public_metrics_following_count': 'following_count',\n",
        "                                    'public_metrics_listed_count': 'listed_count', 'public_metrics_tweet_count': 'tweet_count'})\n",
        "\n",
        "df_user = df_user[['name', 'user_id', 'username', 'date_joined', 'bio', 'location', 'verified',\n",
        "                     'verified_type','follower_count','following_count','listed_count','tweet_count']]\n",
        "df_user.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "5gDtIBwR8uQT",
        "outputId": "291c158a-f16e-4e8d-90d8-79c4db3706c6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9122752d-5fb4-4a3a-819b-6a01f4940d49\", \"Users.csv\", 2058696)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#download the user dataframe as a .csv file\n",
        "df_user.to_csv('Users.csv', index=False)\n",
        "files.download('Users.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##UsersNotCollected"
      ],
      "metadata": {
        "id": "_e49n-jF3BFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "users_to_collect = pd.read_csv('/content/UsersNotCollectedWeek5.csv')"
      ],
      "metadata": {
        "id": "SY90PBql3DXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users_to_collect.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSjVvOEZcTUh",
        "outputId": "d61f28e0-4f9f-4908-fd51-7fb45a756a07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_list = []"
      ],
      "metadata": {
        "id": "bz9kDIjj4AX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# collect data of the users\n",
        "for i in range(len(users_to_collect)):\n",
        "  users = client.get_user(id = users_to_collect.iloc[i,0],\n",
        "                          user_fields = [\"created_at\", \"description\", \"id\", \"location\",\n",
        "                                         \"name\", \"public_metrics\", \"username\", \"verified\", \"verified_type\"])\n",
        "  if users.data != None:\n",
        "    user_list.append(users.data.data)\n",
        "\n",
        "df_user = pd.json_normalize(user_list, sep=\"_\" )\n",
        "df_user.head()\n",
        "\n",
        "#rearrange and rename columns\n",
        "df_user = df_user.rename(columns={'created_at': 'date_joined', 'description': 'bio', 'id': 'user_id',\n",
        "                                    'public_metrics_followers_count': 'follower_count', 'public_metrics_following_count': 'following_count',\n",
        "                                    'public_metrics_listed_count': 'listed_count', 'public_metrics_tweet_count': 'tweet_count'})\n",
        "\n",
        "df_user = df_user[['name', 'user_id', 'username', 'date_joined', 'bio', 'location', 'verified',\n",
        "                     'verified_type','follower_count','following_count','listed_count','tweet_count']]\n",
        "df_user.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "AW9NZw_y3DaA",
        "outputId": "8c600192-65d5-4d12-b65f-a357a49bb1cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                name              user_id       username  \\\n",
              "0  Yano YuukiüíÄ‡æÄ‡Ω≤ùô∞ùöõùöé ùöàùöòùöû ùô≥ùöéùöäùöç ùöàùöéùöù?üíô‡æÄ‡Ω≤  1473630144258789387  ocsidlive1101   \n",
              "\n",
              "                date_joined  \\\n",
              "0  2021-12-22T12:23:11.000Z   \n",
              "\n",
              "                                                 bio location  verified  \\\n",
              "0  üíé @skilletmusic üé§ nu metal, melodic death meta...  „Åß„ÅÉ„Åô„Åì„Éú„Éº„É´     False   \n",
              "\n",
              "  verified_type  follower_count  following_count  listed_count  tweet_count  \n",
              "0          none            2263             5002            11       123013  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-48a796c7-569a-408d-a269-567c90ba74dc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>user_id</th>\n",
              "      <th>username</th>\n",
              "      <th>date_joined</th>\n",
              "      <th>bio</th>\n",
              "      <th>location</th>\n",
              "      <th>verified</th>\n",
              "      <th>verified_type</th>\n",
              "      <th>follower_count</th>\n",
              "      <th>following_count</th>\n",
              "      <th>listed_count</th>\n",
              "      <th>tweet_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Yano YuukiüíÄ‡æÄ‡Ω≤ùô∞ùöõùöé ùöàùöòùöû ùô≥ùöéùöäùöç ùöàùöéùöù?üíô‡æÄ‡Ω≤</td>\n",
              "      <td>1473630144258789387</td>\n",
              "      <td>ocsidlive1101</td>\n",
              "      <td>2021-12-22T12:23:11.000Z</td>\n",
              "      <td>üíé @skilletmusic üé§ nu metal, melodic death meta...</td>\n",
              "      <td>„Åß„ÅÉ„Åô„Åì„Éú„Éº„É´</td>\n",
              "      <td>False</td>\n",
              "      <td>none</td>\n",
              "      <td>2263</td>\n",
              "      <td>5002</td>\n",
              "      <td>11</td>\n",
              "      <td>123013</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-48a796c7-569a-408d-a269-567c90ba74dc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-48a796c7-569a-408d-a269-567c90ba74dc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-48a796c7-569a-408d-a269-567c90ba74dc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_user.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7hDN7DB_a-l",
        "outputId": "99e87871-78df-4cf1-df43-bfefda075a16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 12)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#download the user dataframe as a .csv file\n",
        "df_user.to_csv('Users_Not_Collected_Week4.csv', index=False)\n",
        "files.download('Users_Not_Collected_Week4.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "6vg6ZZfs_gSh",
        "outputId": "2785fece-bbaa-4939-98b0-28e1cae29623"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7a1db32a-ea27-4fb2-a229-a35d2592fd55\", \"Users_Not_Collected_Week4.csv\", 422)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EV0dDIx_xU1U"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "rMWwDgz3JlA0",
        "z4aTMiUAwwAH",
        "s0eFO5zjohFP",
        "YArG3FZ5EnI3",
        "bUyfUKQLifK8",
        "4XGpZACgifT_",
        "Wk5qx_r7ifvD",
        "B2bCTp1chB8B",
        "qeLncvKAg2nd"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}